{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ade48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LinkedIn Parquet Dataset Analysis - Starting Setup...\n",
      "üìÅ Target File: /Users/chromatrical/CAREER/Local Linkedin DB/DataBase/USA_filtered.parquet\n",
      "‚≠ê Chunk Size: 50,000 rows\n",
      "üíæ Output Directory: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n",
      "‚úÖ Setup Complete - Output directory created at /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LinkedIn Parquet Dataset Analysis Script\n",
    "Analyzes 15.2GB parquet file with 20M rows for Semantic Talent Finder project\n",
    "\n",
    "This script processes large parquet files in chunks to:\n",
    "1. Extract schema and data type information\n",
    "2. Analyze data quality and completeness\n",
    "3. Generate insights for Java model optimization\n",
    "4. Provide database schema recommendations\n",
    "5. Configure processing pipeline parameters\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Configuration\n",
    "PARQUET_FILE = \"/Users/chromatrical/CAREER/Local Linkedin DB/DataBase/USA_filtered.parquet\"\n",
    "CHUNK_SIZE = 50000  # Process 50k rows at a time to manage memory\n",
    "OUTPUT_DIR = \"/Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\"\n",
    "\n",
    "print(\"üöÄ LinkedIn Parquet Dataset Analysis - Starting Setup...\")\n",
    "print(f\"üìÅ Target File: {PARQUET_FILE}\")\n",
    "print(f\"‚≠ê Chunk Size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"üíæ Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Setup Complete - Output directory created at {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92kajbfvhzl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LinkedInDataAnalyzer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "class LinkedInDataAnalyzer:\n",
    "    def __init__(self, parquet_file_path, chunk_size=50000):\n",
    "        self.parquet_file = parquet_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.insights = {\n",
    "            'schema_analysis': {},\n",
    "            'data_quality': {},\n",
    "            'content_analysis': {},\n",
    "            'business_logic': {},\n",
    "            'processing_recommendations': {},\n",
    "            'database_schema': {}\n",
    "        }\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Monitor memory usage during processing\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    def analyze_parquet_schema(self):\n",
    "        \"\"\"Analyze parquet file schema and metadata\"\"\"\n",
    "        print(\"üîç Analyzing Parquet Schema...\")\n",
    "        \n",
    "        try:\n",
    "            # Read parquet metadata without loading data\n",
    "            parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "            schema = parquet_file.schema_arrow\n",
    "            metadata = parquet_file.metadata\n",
    "            \n",
    "            # Extract schema information\n",
    "            schema_info = {}\n",
    "            for i, field in enumerate(schema):\n",
    "                schema_info[field.name] = {\n",
    "                    'type': str(field.type),\n",
    "                    'nullable': field.nullable,\n",
    "                    'index': i\n",
    "                }\n",
    "            \n",
    "            self.insights['schema_analysis'] = {\n",
    "                'total_columns': len(schema),\n",
    "                'total_rows': metadata.num_rows,\n",
    "                'file_size_gb': round(os.path.getsize(self.parquet_file) / (1024**3), 2),\n",
    "                'columns': schema_info,\n",
    "                'column_names': [field.name for field in schema]\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Schema Analysis Complete:\")\n",
    "            print(f\"   - Total Rows: {metadata.num_rows:,}\")\n",
    "            print(f\"   - Total Columns: {len(schema)}\")\n",
    "            print(f\"   - File Size: {self.insights['schema_analysis']['file_size_gb']} GB\")\n",
    "            print(f\"   - Columns: {', '.join(list(schema_info.keys())[:10])}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Schema analysis failed: {e}\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = LinkedInDataAnalyzer(PARQUET_FILE, CHUNK_SIZE)\n",
    "print(\"üìä LinkedInDataAnalyzer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "y988owpel2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing Parquet Schema...\n",
      "‚úÖ Schema Analysis Complete:\n",
      "   - Total Rows: 51,352,619\n",
      "   - Total Columns: 62\n",
      "   - File Size: 15.15 GB\n",
      "   - Columns: Full name, Industry, Job title, Sub Role, Industry 2, Emails, Mobile, Phone numbers, Company Name, Company Industry...\n",
      "\n",
      "üìã Schema Summary:\n",
      "Total Rows: 51,352,619\n",
      "Total Columns: 62\n",
      "File Size: 15.15 GB\n",
      "\n",
      "üîç Column Overview:\n",
      "   1. Full name                      | string          | nullable\n",
      "   2. Industry                       | string          | nullable\n",
      "   3. Job title                      | string          | nullable\n",
      "   4. Sub Role                       | string          | nullable\n",
      "   5. Industry 2                     | string          | nullable\n",
      "   6. Emails                         | string          | nullable\n",
      "   7. Mobile                         | string          | nullable\n",
      "   8. Phone numbers                  | string          | nullable\n",
      "   9. Company Name                   | string          | nullable\n",
      "  10. Company Industry               | string          | nullable\n",
      "  11. Company Website                | string          | nullable\n",
      "  12. Company Size                   | string          | nullable\n",
      "  13. Company Founded                | string          | nullable\n",
      "  14. Location                       | string          | nullable\n",
      "  15. Locality                       | string          | nullable\n",
      "  ... and 47 more columns\n"
     ]
    }
   ],
   "source": [
    "# Run schema analysis\n",
    "analyzer.analyze_parquet_schema()\n",
    "\n",
    "# Display schema results\n",
    "print(\"\\nüìã Schema Summary:\")\n",
    "schema_info = analyzer.insights['schema_analysis']\n",
    "print(f\"Total Rows: {schema_info.get('total_rows', 0):,}\")\n",
    "print(f\"Total Columns: {schema_info.get('total_columns', 0)}\")\n",
    "print(f\"File Size: {schema_info.get('file_size_gb', 0)} GB\")\n",
    "\n",
    "print(\"\\nüîç Column Overview:\")\n",
    "columns = schema_info.get('columns', {})\n",
    "for i, (col_name, col_info) in enumerate(list(columns.items())[:15]):  # Show first 15 columns\n",
    "    nullable = \"nullable\" if col_info.get('nullable', True) else \"not null\"\n",
    "    print(f\"  {i+1:2d}. {col_name:<30} | {col_info.get('type', 'unknown'):<15} | {nullable}\")\n",
    "\n",
    "if len(columns) > 15:\n",
    "    print(f\"  ... and {len(columns) - 15} more columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2vywwbzjadq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing Data Quality in Chunks...\n",
      "‚úÖ Data Quality Analysis Complete:\n",
      "   - Rows Analyzed: 200,000\n",
      "   - High Quality Fields: 12\n",
      "   - Low Quality Fields: 43\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_quality_chunked(self):\n",
    "    \"\"\"Analyze data quality in chunks to handle large file\"\"\"\n",
    "    print(\"\\nüîç Analyzing Data Quality in Chunks...\")\n",
    "    \n",
    "    # Initialize aggregators\n",
    "    null_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "    data_types = {}\n",
    "    \n",
    "    chunk_count = 0\n",
    "    total_rows_processed = 0\n",
    "    \n",
    "    try:\n",
    "        # Process file in chunks\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            chunk_count += 1\n",
    "            total_rows_processed += len(chunk_df)\n",
    "            \n",
    "            # Analyze each column\n",
    "            for column in chunk_df.columns:\n",
    "                # Count nulls\n",
    "                null_counts[column] += chunk_df[column].isnull().sum()\n",
    "                total_counts[column] += len(chunk_df)\n",
    "                \n",
    "                # Store data type\n",
    "                if column not in data_types:\n",
    "                    data_types[column] = str(chunk_df[column].dtype)\n",
    "            \n",
    "            # Memory management\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count % 50 == 0:\n",
    "                print(f\"   Processed {chunk_count} chunks ({total_rows_processed:,} rows)\")\n",
    "                print(f\"   Memory usage: {self.get_memory_usage():.2f} MB\")\n",
    "            \n",
    "            # Limit analysis for demo - analyze first 200k rows\n",
    "            if chunk_count >= 4:\n",
    "                break\n",
    "        \n",
    "        # Calculate null percentages\n",
    "        null_percentages = {}\n",
    "        for column in null_counts:\n",
    "            null_percentages[column] = round((null_counts[column] / total_counts[column]) * 100, 2)\n",
    "        \n",
    "        self.insights['data_quality'] = {\n",
    "            'total_rows_analyzed': total_rows_processed,\n",
    "            'chunks_processed': chunk_count,\n",
    "            'null_counts': dict(null_counts),\n",
    "            'null_percentages': null_percentages,\n",
    "            'data_types': data_types,\n",
    "            'completeness_summary': {\n",
    "                'high_quality_fields': [col for col, pct in null_percentages.items() if pct < 5],\n",
    "                'medium_quality_fields': [col for col, pct in null_percentages.items() if 5 <= pct < 25],\n",
    "                'low_quality_fields': [col for col, pct in null_percentages.items() if pct >= 25]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Data Quality Analysis Complete:\")\n",
    "        print(f\"   - Rows Analyzed: {total_rows_processed:,}\")\n",
    "        print(f\"   - High Quality Fields: {len(self.insights['data_quality']['completeness_summary']['high_quality_fields'])}\")\n",
    "        print(f\"   - Low Quality Fields: {len(self.insights['data_quality']['completeness_summary']['low_quality_fields'])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data quality analysis failed: {e}\")\n",
    "\n",
    "# Add method to analyzer class\n",
    "LinkedInDataAnalyzer.analyze_data_quality_chunked = analyze_data_quality_chunked\n",
    "\n",
    "# Run data quality analysis\n",
    "analyzer.analyze_data_quality_chunked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vwpvdw292t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing Data Quality in Chunks...\n",
      "‚úÖ Data Quality Analysis Complete:\n",
      "   - Rows Analyzed: 200,000\n",
      "   - High Quality Fields: 12\n",
      "   - Low Quality Fields: 43\n"
     ]
    }
   ],
   "source": [
    "# Run data quality analysis\n",
    "analyzer.analyze_data_quality_chunked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5si5lmcq3r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Quality Analysis Results:\n",
      "\n",
      "Rows Analyzed: 200,000\n",
      "Chunks Processed: 4\n",
      "\n",
      "üü¢ High Quality Fields (12):\n",
      "  ‚úÖ Full name                      |   0.0% null\n",
      "  ‚úÖ Location                       |   0.1% null\n",
      "  ‚úÖ Locality                       |   2.6% null\n",
      "  ‚úÖ Region                         |   2.2% null\n",
      "  ‚úÖ First Name                     |   0.1% null\n",
      "  ‚úÖ Last Name                      |   0.1% null\n",
      "  ‚úÖ LinkedIn Url                   |   0.1% null\n",
      "  ‚úÖ LinkedIn Username              |   0.1% null\n",
      "  ‚úÖ Location Country               |   0.1% null\n",
      "  ‚úÖ Location Continent             |   0.1% null\n",
      "\n",
      "üü° Medium Quality Fields (7):\n",
      "  ‚ö†Ô∏è  Industry                       |  13.8% null\n",
      "  ‚ö†Ô∏è  Job title                      |  17.0% null\n",
      "  ‚ö†Ô∏è  Metro                          |  11.2% null\n",
      "  ‚ö†Ô∏è  Gender                         |  16.1% null\n",
      "  ‚ö†Ô∏è  Last Updated                   |  18.6% null\n",
      "\n",
      "üî¥ Low Quality Fields (43):\n",
      "  ‚ùå Sub Role                       |  76.1% null\n",
      "  ‚ùå Industry 2                     |  60.8% null\n",
      "  ‚ùå Emails                         |  54.4% null\n",
      "  ‚ùå Mobile                         |  94.7% null\n",
      "  ‚ùå Phone numbers                  |  84.6% null\n"
     ]
    }
   ],
   "source": [
    "# Display data quality results\n",
    "print(\"\\nüìä Data Quality Analysis Results:\")\n",
    "quality_data = analyzer.insights['data_quality']\n",
    "\n",
    "print(f\"\\nRows Analyzed: {quality_data.get('total_rows_analyzed', 0):,}\")\n",
    "print(f\"Chunks Processed: {quality_data.get('chunks_processed', 0)}\")\n",
    "\n",
    "# Show field quality breakdown\n",
    "completeness = quality_data.get('completeness_summary', {})\n",
    "print(f\"\\nüü¢ High Quality Fields ({len(completeness.get('high_quality_fields', []))}):\")\n",
    "for field in completeness.get('high_quality_fields', [])[:10]:\n",
    "    null_pct = quality_data.get('null_percentages', {}).get(field, 0)\n",
    "    print(f\"  ‚úÖ {field:<30} | {null_pct:5.1f}% null\")\n",
    "\n",
    "print(f\"\\nüü° Medium Quality Fields ({len(completeness.get('medium_quality_fields', []))}):\")\n",
    "for field in completeness.get('medium_quality_fields', [])[:5]:\n",
    "    null_pct = quality_data.get('null_percentages', {}).get(field, 0)\n",
    "    print(f\"  ‚ö†Ô∏è  {field:<30} | {null_pct:5.1f}% null\")\n",
    "\n",
    "print(f\"\\nüî¥ Low Quality Fields ({len(completeness.get('low_quality_fields', []))}):\")\n",
    "for field in completeness.get('low_quality_fields', [])[:5]:\n",
    "    null_pct = quality_data.get('null_percentages', {}).get(field, 0)\n",
    "    print(f\"  ‚ùå {field:<30} | {null_pct:5.1f}% null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xpr47emkvsl",
   "metadata": {},
   "outputs": [],
   "source": "# First add the missing methods to the analyzer class\ndef generate_java_recommendations(self):\n    \"\"\"Generate Java model and configuration recommendations\"\"\"\n    print(\"\\nüîç Generating Java Recommendations...\")\n    \n    schema = self.insights.get('schema_analysis', {})\n    quality = self.insights.get('data_quality', {})\n    content = self.insights.get('content_analysis', {})\n    \n    # Generate Java field recommendations\n    java_fields = {}\n    for col_name, col_info in schema.get('columns', {}).items():\n        field_name = self.to_camel_case(col_name)\n        \n        # Determine Java type and constraints\n        if 'string' in col_info['type'].lower() or 'object' in col_info['type'].lower():\n            # Get length recommendation from content analysis\n            max_length = 255  # default\n            if col_name in content.get('content_stats', {}):\n                max_length = max(500, content['content_stats'][col_name].get('percentile_95', 255))\n            \n            java_fields[field_name] = {\n                'original_column': col_name,\n                'java_type': 'String',\n                'jpa_annotation': f'@Column(name = \"{col_name}\", length = {max_length})',\n                'nullable': col_info.get('nullable', True),\n                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n            }\n        \n        elif 'int' in col_info['type'].lower():\n            java_fields[field_name] = {\n                'original_column': col_name,\n                'java_type': 'Integer',\n                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n                'nullable': col_info.get('nullable', True),\n                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n            }\n        \n        elif 'bool' in col_info['type'].lower():\n            java_fields[field_name] = {\n                'original_column': col_name,\n                'java_type': 'Boolean',\n                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n                'nullable': col_info.get('nullable', True),\n                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n            }\n    \n    # Processing recommendations\n    processing_config = {\n        'recommended_batch_size': min(5000, max(1000, self.chunk_size // 10)),\n        'memory_per_batch_mb': round(self.get_memory_usage() / 10, 2),\n        'estimated_processing_time_hours': round((schema.get('total_rows', 0) / 10000) / 60, 2),\n        'high_priority_fields': quality.get('completeness_summary', {}).get('high_quality_fields', []),\n        'validation_required_fields': quality.get('completeness_summary', {}).get('low_quality_fields', [])\n    }\n    \n    self.insights['processing_recommendations'] = {\n        'java_fields': java_fields,\n        'processing_config': processing_config\n    }\n    \n    print(f\"‚úÖ Java Recommendations Generated:\")\n    print(f\"   - Java Fields: {len(java_fields)}\")\n    print(f\"   - Recommended Batch Size: {processing_config['recommended_batch_size']}\")\n\ndef to_camel_case(self, snake_str):\n    \"\"\"Convert snake_case to camelCase\"\"\"\n    components = snake_str.split('_')\n    return components[0] + ''.join(word.capitalize() for word in components[1:])\n\ndef generate_database_schema(self):\n    \"\"\"Generate optimized database schema\"\"\"\n    print(\"\\nüîç Generating Database Schema...\")\n    \n    java_fields = self.insights.get('processing_recommendations', {}).get('java_fields', {})\n    \n    # Generate CREATE TABLE statement\n    create_table = \"CREATE TABLE profiles (\\n\"\n    create_table += \"    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\\n\"\n    \n    for field_name, field_info in list(java_fields.items())[:30]:  # Limit to first 30 fields for demo\n        col_name = field_info['original_column']\n        java_type = field_info['java_type']\n        null_pct = field_info['null_percentage']\n        \n        # Determine SQL type and constraints\n        if java_type == 'String':\n            length = field_info['jpa_annotation'].split('length = ')[1].split(')')[0] if 'length =' in field_info['jpa_annotation'] else '255'\n            sql_type = f\"VARCHAR({length})\"\n        elif java_type == 'Integer':\n            sql_type = \"INTEGER\"\n        elif java_type == 'Boolean':\n            sql_type = \"BOOLEAN\"\n        else:\n            sql_type = \"TEXT\"\n        \n        # Add NOT NULL for high-quality fields\n        nullable = \"\" if null_pct > 10 else \" NOT NULL\" if null_pct < 1 else \"\"\n        \n        create_table += f\"    {col_name} {sql_type}{nullable},\\n\"\n    \n    # Add vector embedding column\n    create_table += \"    embedding vector(1536),\\n\"\n    create_table += \"    created_at TIMESTAMP DEFAULT NOW(),\\n\"\n    create_table += \"    updated_at TIMESTAMP DEFAULT NOW()\\n\"\n    create_table += \");\"\n    \n    # Generate indexes\n    indexes = []\n    indexes.append(\"CREATE INDEX CONCURRENTLY profiles_embedding_hnsw_idx ON profiles USING hnsw (embedding vector_cosine_ops);\")\n    \n    # Add indexes for high-quality, commonly queried fields\n    high_quality_fields = self.insights.get('data_quality', {}).get('completeness_summary', {}).get('high_quality_fields', [])\n    for field in high_quality_fields[:5]:  # Top 5 fields\n        if field != 'id':\n            indexes.append(f\"CREATE INDEX CONCURRENTLY idx_profiles_{field} ON profiles({field});\")\n    \n    self.insights['database_schema'] = {\n        'create_table_sql': create_table,\n        'indexes_sql': indexes\n    }\n    \n    print(f\"‚úÖ Database Schema Generated\")\n\ndef save_insights(self):\n    \"\"\"Save all insights to files\"\"\"\n    print(\"\\nüíæ Saving Analysis Results...\")\n    \n    # Save JSON insights\n    insights_file = os.path.join(OUTPUT_DIR, 'linkedin_analysis_insights.json')\n    with open(insights_file, 'w') as f:\n        json.dump(self.insights, f, indent=2, default=str)\n    \n    # Save database schema\n    schema_file = os.path.join(OUTPUT_DIR, 'optimized_schema.sql')\n    with open(schema_file, 'w') as f:\n        f.write(self.insights.get('database_schema', {}).get('create_table_sql', ''))\n        f.write('\\n\\n-- Indexes\\n')\n        for index in self.insights.get('database_schema', {}).get('indexes_sql', []):\n            f.write(index + '\\n')\n    \n    print(f\"‚úÖ Results Saved to: {OUTPUT_DIR}\")\n    print(f\"   - Insights: linkedin_analysis_insights.json\")\n    print(f\"   - Database Schema: optimized_schema.sql\")\n    \n    return insights_file, schema_file\n\n# Add methods to analyzer class\nLinkedInDataAnalyzer.generate_java_recommendations = generate_java_recommendations\nLinkedInDataAnalyzer.to_camel_case = to_camel_case\nLinkedInDataAnalyzer.generate_database_schema = generate_database_schema\nLinkedInDataAnalyzer.save_insights = save_insights\n\n# Now generate final insights and outputs\nanalyzer.generate_java_recommendations()\nanalyzer.generate_database_schema()\ninsights_file, schema_file = analyzer.save_insights()\n\n# Display final summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéâ LINKEDIN DATASET ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\nschema_info = analyzer.insights['schema_analysis']\nquality_info = analyzer.insights['data_quality']\nprocessing_info = analyzer.insights['processing_recommendations']['processing_config']\n\nprint(f\"\\nüìä DATASET OVERVIEW:\")\nprint(f\"   üìÅ File Size: {schema_info.get('file_size_gb', 0)} GB\")\nprint(f\"   üìã Total Rows: {schema_info.get('total_rows', 0):,}\")\nprint(f\"   üóÇÔ∏è  Total Columns: {schema_info.get('total_columns', 0)}\")\nprint(f\"   üîç Rows Analyzed: {quality_info.get('total_rows_analyzed', 0):,}\")\n\nprint(f\"\\nüéØ DATA QUALITY SUMMARY:\")\ncompleteness = quality_info.get('completeness_summary', {})\nprint(f\"   üü¢ High Quality Fields: {len(completeness.get('high_quality_fields', []))}\")\nprint(f\"   üü° Medium Quality Fields: {len(completeness.get('medium_quality_fields', []))}\")\nprint(f\"   üî¥ Low Quality Fields: {len(completeness.get('low_quality_fields', []))}\")\n\nprint(f\"\\n‚öôÔ∏è PROCESSING RECOMMENDATIONS:\")\nprint(f\"   üì¶ Recommended Batch Size: {processing_info.get('recommended_batch_size', 0):,}\")\nprint(f\"   üíæ Memory per Batch: {processing_info.get('memory_per_batch_mb', 0):.1f} MB\")\nprint(f\"   ‚è±Ô∏è  Est. Processing Time: {processing_info.get('estimated_processing_time_hours', 0):.1f} hours\")\n\nprint(f\"\\nüìÅ OUTPUT FILES GENERATED:\")\nprint(f\"   üìã Analysis Report: {insights_file}\")\nprint(f\"   üóÉÔ∏è  Database Schema: {schema_file}\")\n\nprint(f\"\\nüöÄ NEXT STEPS:\")\nprint(\"   1. Review the generated analysis files\")\nprint(\"   2. Use the database schema for your PostgreSQL setup\")\nprint(\"   3. Apply the processing recommendations to your Java application\")\nprint(\"   4. Use the identified high-quality fields for core functionality\")\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tnsu913qak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Methods added to LinkedInDataAnalyzer class\n"
     ]
    }
   ],
   "source": [
    "def generate_java_recommendations(self):\n",
    "    \"\"\"Generate Java model and configuration recommendations\"\"\"\n",
    "    print(\"\\nüîç Generating Java Recommendations...\")\n",
    "    \n",
    "    schema = self.insights.get('schema_analysis', {})\n",
    "    quality = self.insights.get('data_quality', {})\n",
    "    content = self.insights.get('content_analysis', {})\n",
    "    \n",
    "    # Generate Java field recommendations\n",
    "    java_fields = {}\n",
    "    for col_name, col_info in schema.get('columns', {}).items():\n",
    "        field_name = self.to_camel_case(col_name)\n",
    "        \n",
    "        # Determine Java type and constraints\n",
    "        if 'string' in col_info['type'].lower() or 'object' in col_info['type'].lower():\n",
    "            # Get length recommendation from content analysis\n",
    "            max_length = 255  # default\n",
    "            if col_name in content.get('content_stats', {}):\n",
    "                max_length = max(500, content['content_stats'][col_name].get('percentile_95', 255))\n",
    "            \n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'String',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\", length = {max_length})',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "        \n",
    "        elif 'int' in col_info['type'].lower():\n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'Integer',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "        \n",
    "        elif 'bool' in col_info['type'].lower():\n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'Boolean',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "    \n",
    "    # Processing recommendations\n",
    "    processing_config = {\n",
    "        'recommended_batch_size': min(5000, max(1000, self.chunk_size // 10)),\n",
    "        'memory_per_batch_mb': round(self.get_memory_usage() / 10, 2),\n",
    "        'estimated_processing_time_hours': round((schema.get('total_rows', 0) / 10000) / 60, 2),\n",
    "        'high_priority_fields': quality.get('completeness_summary', {}).get('high_quality_fields', []),\n",
    "        'validation_required_fields': quality.get('completeness_summary', {}).get('low_quality_fields', [])\n",
    "    }\n",
    "    \n",
    "    self.insights['processing_recommendations'] = {\n",
    "        'java_fields': java_fields,\n",
    "        'processing_config': processing_config\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Java Recommendations Generated:\")\n",
    "    print(f\"   - Java Fields: {len(java_fields)}\")\n",
    "    print(f\"   - Recommended Batch Size: {processing_config['recommended_batch_size']}\")\n",
    "\n",
    "def to_camel_case(self, snake_str):\n",
    "    \"\"\"Convert snake_case to camelCase\"\"\"\n",
    "    components = snake_str.split('_')\n",
    "    return components[0] + ''.join(word.capitalize() for word in components[1:])\n",
    "\n",
    "def generate_database_schema(self):\n",
    "    \"\"\"Generate optimized database schema\"\"\"\n",
    "    print(\"\\nüîç Generating Database Schema...\")\n",
    "    \n",
    "    java_fields = self.insights.get('processing_recommendations', {}).get('java_fields', {})\n",
    "    \n",
    "    # Generate CREATE TABLE statement\n",
    "    create_table = \"CREATE TABLE profiles (\\n\"\n",
    "    create_table += \"    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\\n\"\n",
    "    \n",
    "    for field_name, field_info in list(java_fields.items())[:30]:  # Limit to first 30 fields for demo\n",
    "        col_name = field_info['original_column']\n",
    "        java_type = field_info['java_type']\n",
    "        null_pct = field_info['null_percentage']\n",
    "        \n",
    "        # Determine SQL type and constraints\n",
    "        if java_type == 'String':\n",
    "            length = field_info['jpa_annotation'].split('length = ')[1].split(')')[0] if 'length =' in field_info['jpa_annotation'] else '255'\n",
    "            sql_type = f\"VARCHAR({length})\"\n",
    "        elif java_type == 'Integer':\n",
    "            sql_type = \"INTEGER\"\n",
    "        elif java_type == 'Boolean':\n",
    "            sql_type = \"BOOLEAN\"\n",
    "        else:\n",
    "            sql_type = \"TEXT\"\n",
    "        \n",
    "        # Add NOT NULL for high-quality fields\n",
    "        nullable = \"\" if null_pct > 10 else \" NOT NULL\" if null_pct < 1 else \"\"\n",
    "        \n",
    "        create_table += f\"    {col_name} {sql_type}{nullable},\\n\"\n",
    "    \n",
    "    # Add vector embedding column\n",
    "    create_table += \"    embedding vector(1536),\\n\"\n",
    "    create_table += \"    created_at TIMESTAMP DEFAULT NOW(),\\n\"\n",
    "    create_table += \"    updated_at TIMESTAMP DEFAULT NOW()\\n\"\n",
    "    create_table += \");\"\n",
    "    \n",
    "    # Generate indexes\n",
    "    indexes = []\n",
    "    indexes.append(\"CREATE INDEX CONCURRENTLY profiles_embedding_hnsw_idx ON profiles USING hnsw (embedding vector_cosine_ops);\")\n",
    "    \n",
    "    # Add indexes for high-quality, commonly queried fields\n",
    "    high_quality_fields = self.insights.get('data_quality', {}).get('completeness_summary', {}).get('high_quality_fields', [])\n",
    "    for field in high_quality_fields[:5]:  # Top 5 fields\n",
    "        if field != 'id':\n",
    "            indexes.append(f\"CREATE INDEX CONCURRENTLY idx_profiles_{field} ON profiles({field});\")\n",
    "    \n",
    "    self.insights['database_schema'] = {\n",
    "        'create_table_sql': create_table,\n",
    "        'indexes_sql': indexes\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Database Schema Generated\")\n",
    "\n",
    "def save_insights(self):\n",
    "    \"\"\"Save all insights to files\"\"\"\n",
    "    print(\"\\nüíæ Saving Analysis Results...\")\n",
    "    \n",
    "    # Save JSON insights\n",
    "    insights_file = os.path.join(OUTPUT_DIR, 'linkedin_analysis_insights.json')\n",
    "    with open(insights_file, 'w') as f:\n",
    "        json.dump(self.insights, f, indent=2, default=str)\n",
    "    \n",
    "    # Save database schema\n",
    "    schema_file = os.path.join(OUTPUT_DIR, 'optimized_schema.sql')\n",
    "    with open(schema_file, 'w') as f:\n",
    "        f.write(self.insights.get('database_schema', {}).get('create_table_sql', ''))\n",
    "        f.write('\\n\\n-- Indexes\\n')\n",
    "        for index in self.insights.get('database_schema', {}).get('indexes_sql', []):\n",
    "            f.write(index + '\\n')\n",
    "    \n",
    "    print(f\"‚úÖ Results Saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"   - Insights: linkedin_analysis_insights.json\")\n",
    "    print(f\"   - Database Schema: optimized_schema.sql\")\n",
    "    \n",
    "    return insights_file, schema_file\n",
    "\n",
    "# Add methods to analyzer class\n",
    "LinkedInDataAnalyzer.generate_java_recommendations = generate_java_recommendations\n",
    "LinkedInDataAnalyzer.to_camel_case = to_camel_case\n",
    "LinkedInDataAnalyzer.generate_database_schema = generate_database_schema\n",
    "LinkedInDataAnalyzer.save_insights = save_insights\n",
    "\n",
    "print(\"‚úÖ Methods added to LinkedInDataAnalyzer class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hioyr52pq9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Generating Java Recommendations...\n",
      "‚úÖ Java Recommendations Generated:\n",
      "   - Java Fields: 62\n",
      "   - Recommended Batch Size: 5000\n",
      "\n",
      "üîç Generating Database Schema...\n",
      "‚úÖ Database Schema Generated\n",
      "\n",
      "üíæ Saving Analysis Results...\n",
      "‚úÖ Results Saved to: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n",
      "   - Insights: linkedin_analysis_insights.json\n",
      "   - Database Schema: optimized_schema.sql\n",
      "\n",
      "================================================================================\n",
      "üéâ LINKEDIN DATASET ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "   üìÅ File Size: 15.15 GB\n",
      "   üìã Total Rows: 51,352,619\n",
      "   üóÇÔ∏è  Total Columns: 62\n",
      "   üîç Rows Analyzed: 200,000\n",
      "\n",
      "üéØ DATA QUALITY SUMMARY:\n",
      "   üü¢ High Quality Fields: 12\n",
      "   üü° Medium Quality Fields: 7\n",
      "   üî¥ Low Quality Fields: 43\n",
      "\n",
      "‚öôÔ∏è PROCESSING RECOMMENDATIONS:\n",
      "   üì¶ Recommended Batch Size: 5,000\n",
      "   üíæ Memory per Batch: 5.7 MB\n",
      "   ‚è±Ô∏è  Est. Processing Time: 85.6 hours\n",
      "\n",
      "üìÅ OUTPUT FILES GENERATED:\n",
      "   üìã Analysis Report: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/linkedin_analysis_insights.json\n",
      "   üóÉÔ∏è  Database Schema: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/optimized_schema.sql\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1. Review the generated analysis files\n",
      "   2. Use the database schema for your PostgreSQL setup\n",
      "   3. Apply the processing recommendations to your Java application\n",
      "   4. Use the identified high-quality fields for core functionality\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Now run the complete analysis pipeline\n",
    "analyzer.generate_java_recommendations()\n",
    "analyzer.generate_database_schema()\n",
    "insights_file, schema_file = analyzer.save_insights()\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ LINKEDIN DATASET ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "schema_info = analyzer.insights['schema_analysis']\n",
    "quality_info = analyzer.insights['data_quality']\n",
    "processing_info = analyzer.insights['processing_recommendations']['processing_config']\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   üìÅ File Size: {schema_info.get('file_size_gb', 0)} GB\")\n",
    "print(f\"   üìã Total Rows: {schema_info.get('total_rows', 0):,}\")\n",
    "print(f\"   üóÇÔ∏è  Total Columns: {schema_info.get('total_columns', 0)}\")\n",
    "print(f\"   üîç Rows Analyzed: {quality_info.get('total_rows_analyzed', 0):,}\")\n",
    "\n",
    "print(f\"\\nüéØ DATA QUALITY SUMMARY:\")\n",
    "completeness = quality_info.get('completeness_summary', {})\n",
    "print(f\"   üü¢ High Quality Fields: {len(completeness.get('high_quality_fields', []))}\")\n",
    "print(f\"   üü° Medium Quality Fields: {len(completeness.get('medium_quality_fields', []))}\")\n",
    "print(f\"   üî¥ Low Quality Fields: {len(completeness.get('low_quality_fields', []))}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è PROCESSING RECOMMENDATIONS:\")\n",
    "print(f\"   üì¶ Recommended Batch Size: {processing_info.get('recommended_batch_size', 0):,}\")\n",
    "print(f\"   üíæ Memory per Batch: {processing_info.get('memory_per_batch_mb', 0):.1f} MB\")\n",
    "print(f\"   ‚è±Ô∏è  Est. Processing Time: {processing_info.get('estimated_processing_time_hours', 0):.1f} hours\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   üìã Analysis Report: {insights_file}\")\n",
    "print(f\"   üóÉÔ∏è  Database Schema: {schema_file}\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   1. Review the generated analysis files\")\n",
    "print(\"   2. Use the database schema for your PostgreSQL setup\")\n",
    "print(\"   3. Apply the processing recommendations to your Java application\")\n",
    "print(\"   4. Use the identified high-quality fields for core functionality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}