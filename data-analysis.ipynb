{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1ade48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LinkedIn Parquet Dataset Analysis - Starting Setup...\n",
      "üìÅ Target File: /Users/chromatrical/CAREER/Local Linkedin DB/DataBase/USA_filtered.parquet\n",
      "‚≠ê Chunk Size: 50,000 rows\n",
      "üíæ Output Directory: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n",
      "‚úÖ Setup Complete - Output directory created at /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LinkedIn Parquet Dataset Analysis Script\n",
    "Analyzes 15.2GB parquet file with 20M rows for Semantic Talent Finder project\n",
    "\n",
    "This script processes large parquet files in chunks to:\n",
    "1. Extract schema and data type information\n",
    "2. Analyze data quality and completeness\n",
    "3. Generate insights for Java model optimization\n",
    "4. Provide database schema recommendations\n",
    "5. Configure processing pipeline parameters\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Configuration\n",
    "PARQUET_FILE = \"/Users/chromatrical/CAREER/Local Linkedin DB/DataBase/USA_filtered.parquet\"\n",
    "CHUNK_SIZE = 50000  # Process 50k rows at a time to manage memory\n",
    "OUTPUT_DIR = \"/Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\"\n",
    "\n",
    "print(\"üöÄ LinkedIn Parquet Dataset Analysis - Starting Setup...\")\n",
    "print(f\"üìÅ Target File: {PARQUET_FILE}\")\n",
    "print(f\"‚≠ê Chunk Size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"üíæ Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Setup Complete - Output directory created at {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92kajbfvhzl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LinkedInDataAnalyzer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "class LinkedInDataAnalyzer:\n",
    "    def __init__(self, parquet_file_path, chunk_size=50000):\n",
    "        self.parquet_file = parquet_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.insights = {\n",
    "            'schema_analysis': {},\n",
    "            'data_quality': {},\n",
    "            'content_analysis': {},\n",
    "            'business_logic': {},\n",
    "            'processing_recommendations': {},\n",
    "            'database_schema': {}\n",
    "        }\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Monitor memory usage during processing\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    def analyze_parquet_schema(self):\n",
    "        \"\"\"Analyze parquet file schema and metadata\"\"\"\n",
    "        print(\"üîç Analyzing Parquet Schema...\")\n",
    "        \n",
    "        try:\n",
    "            # Read parquet metadata without loading data\n",
    "            parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "            schema = parquet_file.schema_arrow\n",
    "            metadata = parquet_file.metadata\n",
    "            \n",
    "            # Extract schema information\n",
    "            schema_info = {}\n",
    "            for i, field in enumerate(schema):\n",
    "                schema_info[field.name] = {\n",
    "                    'type': str(field.type),\n",
    "                    'nullable': field.nullable,\n",
    "                    'index': i\n",
    "                }\n",
    "            \n",
    "            self.insights['schema_analysis'] = {\n",
    "                'total_columns': len(schema),\n",
    "                'total_rows': metadata.num_rows,\n",
    "                'file_size_gb': round(os.path.getsize(self.parquet_file) / (1024**3), 2),\n",
    "                'columns': schema_info,\n",
    "                'column_names': [field.name for field in schema]\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Schema Analysis Complete:\")\n",
    "            print(f\"   - Total Rows: {metadata.num_rows:,}\")\n",
    "            print(f\"   - Total Columns: {len(schema)}\")\n",
    "            print(f\"   - File Size: {self.insights['schema_analysis']['file_size_gb']} GB\")\n",
    "            print(f\"   - Columns: {', '.join(list(schema_info.keys())[:10])}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Schema analysis failed: {e}\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = LinkedInDataAnalyzer(PARQUET_FILE, CHUNK_SIZE)\n",
    "print(\"üìä LinkedInDataAnalyzer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "y988owpel2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing Parquet Schema...\n",
      "‚úÖ Schema Analysis Complete:\n",
      "   - Total Rows: 51,352,619\n",
      "   - Total Columns: 62\n",
      "   - File Size: 15.15 GB\n",
      "   - Columns: Full name, Industry, Job title, Sub Role, Industry 2, Emails, Mobile, Phone numbers, Company Name, Company Industry...\n",
      "\n",
      "üìã Schema Summary:\n",
      "Total Rows: 51,352,619\n",
      "Total Columns: 62\n",
      "File Size: 15.15 GB\n",
      "\n",
      "üîç Column Overview:\n",
      "   1. Full name                      | string          | nullable\n",
      "   2. Industry                       | string          | nullable\n",
      "   3. Job title                      | string          | nullable\n",
      "   4. Sub Role                       | string          | nullable\n",
      "   5. Industry 2                     | string          | nullable\n",
      "   6. Emails                         | string          | nullable\n",
      "   7. Mobile                         | string          | nullable\n",
      "   8. Phone numbers                  | string          | nullable\n",
      "   9. Company Name                   | string          | nullable\n",
      "  10. Company Industry               | string          | nullable\n",
      "  11. Company Website                | string          | nullable\n",
      "  12. Company Size                   | string          | nullable\n",
      "  13. Company Founded                | string          | nullable\n",
      "  14. Location                       | string          | nullable\n",
      "  15. Locality                       | string          | nullable\n",
      "  ... and 47 more columns\n"
     ]
    }
   ],
   "source": [
    "# Run schema analysis\n",
    "analyzer.analyze_parquet_schema()\n",
    "\n",
    "# Display schema results\n",
    "print(\"\\nüìã Schema Summary:\")\n",
    "schema_info = analyzer.insights['schema_analysis']\n",
    "print(f\"Total Rows: {schema_info.get('total_rows', 0):,}\")\n",
    "print(f\"Total Columns: {schema_info.get('total_columns', 0)}\")\n",
    "print(f\"File Size: {schema_info.get('file_size_gb', 0)} GB\")\n",
    "\n",
    "print(\"\\nüîç Column Overview:\")\n",
    "columns = schema_info.get('columns', {})\n",
    "for i, (col_name, col_info) in enumerate(list(columns.items())[:15]):  # Show first 15 columns\n",
    "    nullable = \"nullable\" if col_info.get('nullable', True) else \"not null\"\n",
    "    print(f\"  {i+1:2d}. {col_name:<30} | {col_info.get('type', 'unknown'):<15} | {nullable}\")\n",
    "\n",
    "if len(columns) > 15:\n",
    "    print(f\"  ... and {len(columns) - 15} more columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2vywwbzjadq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing Data Quality in Chunks...\n",
      "‚úÖ Data Quality Analysis Complete:\n",
      "   - Rows Analyzed: 200,000\n",
      "   - High Quality Fields: 12\n",
      "   - Low Quality Fields: 43\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_quality_chunked(self):\n",
    "    \"\"\"Analyze data quality in chunks to handle large file\"\"\"\n",
    "    print(\"\\nüîç Analyzing Data Quality in Chunks...\")\n",
    "    \n",
    "    # Initialize aggregators\n",
    "    null_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "    data_types = {}\n",
    "    \n",
    "    chunk_count = 0\n",
    "    total_rows_processed = 0\n",
    "    \n",
    "    try:\n",
    "        # Process file in chunks\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            chunk_count += 1\n",
    "            total_rows_processed += len(chunk_df)\n",
    "            \n",
    "            # Analyze each column\n",
    "            for column in chunk_df.columns:\n",
    "                # Count nulls\n",
    "                null_counts[column] += chunk_df[column].isnull().sum()\n",
    "                total_counts[column] += len(chunk_df)\n",
    "                \n",
    "                # Store data type\n",
    "                if column not in data_types:\n",
    "                    data_types[column] = str(chunk_df[column].dtype)\n",
    "            \n",
    "            # Memory management\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count % 50 == 0:\n",
    "                print(f\"   Processed {chunk_count} chunks ({total_rows_processed:,} rows)\")\n",
    "                print(f\"   Memory usage: {self.get_memory_usage():.2f} MB\")\n",
    "            \n",
    "            # Limit analysis for demo - analyze first 200k rows\n",
    "            if chunk_count >= 4:\n",
    "                break\n",
    "        \n",
    "        # Calculate null percentages\n",
    "        null_percentages = {}\n",
    "        for column in null_counts:\n",
    "            null_percentages[column] = round((null_counts[column] / total_counts[column]) * 100, 2)\n",
    "        \n",
    "        self.insights['data_quality'] = {\n",
    "            'total_rows_analyzed': total_rows_processed,\n",
    "            'chunks_processed': chunk_count,\n",
    "            'null_counts': dict(null_counts),\n",
    "            'null_percentages': null_percentages,\n",
    "            'data_types': data_types,\n",
    "            'completeness_summary': {\n",
    "                'high_quality_fields': [col for col, pct in null_percentages.items() if pct < 5],\n",
    "                'medium_quality_fields': [col for col, pct in null_percentages.items() if 5 <= pct < 25],\n",
    "                'low_quality_fields': [col for col, pct in null_percentages.items() if pct >= 25]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Data Quality Analysis Complete:\")\n",
    "        print(f\"   - Rows Analyzed: {total_rows_processed:,}\")\n",
    "        print(f\"   - High Quality Fields: {len(self.insights['data_quality']['completeness_summary']['high_quality_fields'])}\")\n",
    "        print(f\"   - Low Quality Fields: {len(self.insights['data_quality']['completeness_summary']['low_quality_fields'])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data quality analysis failed: {e}\")\n",
    "\n",
    "# Add method to analyzer class\n",
    "LinkedInDataAnalyzer.analyze_data_quality_chunked = analyze_data_quality_chunked\n",
    "\n",
    "# Run data quality analysis\n",
    "analyzer.analyze_data_quality_chunked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vwpvdw292t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing Data Quality in Chunks...\n",
      "‚úÖ Data Quality Analysis Complete:\n",
      "   - Rows Analyzed: 200,000\n",
      "   - High Quality Fields: 12\n",
      "   - Low Quality Fields: 43\n"
     ]
    }
   ],
   "source": [
    "# Run data quality analysis\n",
    "analyzer.analyze_data_quality_chunked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5si5lmcq3r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Quality Analysis Results:\n",
      "\n",
      "Rows Analyzed: 200,000\n",
      "Chunks Processed: 4\n",
      "\n",
      "üü¢ High Quality Fields (12):\n",
      "  ‚úÖ Full name                      |   0.0% null\n",
      "  ‚úÖ Location                       |   0.1% null\n",
      "  ‚úÖ Locality                       |   2.6% null\n",
      "  ‚úÖ Region                         |   2.2% null\n",
      "  ‚úÖ First Name                     |   0.1% null\n",
      "  ‚úÖ Last Name                      |   0.1% null\n",
      "  ‚úÖ LinkedIn Url                   |   0.1% null\n",
      "  ‚úÖ LinkedIn Username              |   0.1% null\n",
      "  ‚úÖ Location Country               |   0.1% null\n",
      "  ‚úÖ Location Continent             |   0.1% null\n",
      "\n",
      "üü° Medium Quality Fields (7):\n",
      "  ‚ö†Ô∏è  Industry                       |  13.8% null\n",
      "  ‚ö†Ô∏è  Job title                      |  17.0% null\n",
      "  ‚ö†Ô∏è  Metro                          |  11.2% null\n",
      "  ‚ö†Ô∏è  Gender                         |  16.1% null\n",
      "  ‚ö†Ô∏è  Last Updated                   |  18.6% null\n",
      "\n",
      "üî¥ Low Quality Fields (43):\n",
      "  ‚ùå Sub Role                       |  76.1% null\n",
      "  ‚ùå Industry 2                     |  60.8% null\n",
      "  ‚ùå Emails                         |  54.4% null\n",
      "  ‚ùå Mobile                         |  94.7% null\n",
      "  ‚ùå Phone numbers                  |  84.6% null\n"
     ]
    }
   ],
   "source": [
    "# Display data quality results\n",
    "print(\"\\nüìä Data Quality Analysis Results:\")\n",
    "quality_data = analyzer.insights['data_quality']\n",
    "\n",
    "print(f\"\\nRows Analyzed: {quality_data.get('total_rows_analyzed', 0):,}\")\n",
    "print(f\"Chunks Processed: {quality_data.get('chunks_processed', 0)}\")\n",
    "\n",
    "# Show field quality breakdown\n",
    "completeness = quality_data.get('completeness_summary', {})\n",
    "print(f\"\\nüü¢ High Quality Fields ({len(completeness.get('high_quality_fields', []))}):\")\n",
    "for field in completeness.get('high_quality_fields', [])[:10]:\n",
    "    null_pct = quality_data.get('null_percentages', {}).get(field, 0)\n",
    "    print(f\"  ‚úÖ {field:<30} | {null_pct:5.1f}% null\")\n",
    "\n",
    "print(f\"\\nüü° Medium Quality Fields ({len(completeness.get('medium_quality_fields', []))}):\")\n",
    "for field in completeness.get('medium_quality_fields', [])[:5]:\n",
    "    null_pct = quality_data.get('null_percentages', {}).get(field, 0)\n",
    "    print(f\"  ‚ö†Ô∏è  {field:<30} | {null_pct:5.1f}% null\")\n",
    "\n",
    "print(f\"\\nüî¥ Low Quality Fields ({len(completeness.get('low_quality_fields', []))}):\")\n",
    "for field in completeness.get('low_quality_fields', [])[:5]:\n",
    "    null_pct = quality_data.get('null_percentages', {}).get(field, 0)\n",
    "    print(f\"  ‚ùå {field:<30} | {null_pct:5.1f}% null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "xpr47emkvsl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Generating Java Recommendations...\n",
      "‚úÖ Java Recommendations Generated:\n",
      "   - Java Fields: 62\n",
      "   - Recommended Batch Size: 5000\n",
      "\n",
      "üîç Generating Database Schema...\n",
      "‚úÖ Database Schema Generated\n",
      "\n",
      "üíæ Saving Analysis Results...\n",
      "‚úÖ Results Saved to: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n",
      "   - Insights: linkedin_analysis_insights.json\n",
      "   - Database Schema: optimized_schema.sql\n",
      "\n",
      "================================================================================\n",
      "üéâ LINKEDIN DATASET ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "   üìÅ File Size: 15.15 GB\n",
      "   üìã Total Rows: 51,352,619\n",
      "   üóÇÔ∏è  Total Columns: 62\n",
      "   üîç Rows Analyzed: 200,000\n",
      "\n",
      "üéØ DATA QUALITY SUMMARY:\n",
      "   üü¢ High Quality Fields: 12\n",
      "   üü° Medium Quality Fields: 7\n",
      "   üî¥ Low Quality Fields: 43\n",
      "\n",
      "‚öôÔ∏è PROCESSING RECOMMENDATIONS:\n",
      "   üì¶ Recommended Batch Size: 5,000\n",
      "   üíæ Memory per Batch: 67.2 MB\n",
      "   ‚è±Ô∏è  Est. Processing Time: 85.6 hours\n",
      "\n",
      "üìÅ OUTPUT FILES GENERATED:\n",
      "   üìã Analysis Report: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/linkedin_analysis_insights.json\n",
      "   üóÉÔ∏è  Database Schema: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/optimized_schema.sql\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1. Review the generated analysis files\n",
      "   2. Use the database schema for your PostgreSQL setup\n",
      "   3. Apply the processing recommendations to your Java application\n",
      "   4. Use the identified high-quality fields for core functionality\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# First add the missing methods to the analyzer class\n",
    "def generate_java_recommendations(self):\n",
    "    \"\"\"Generate Java model and configuration recommendations\"\"\"\n",
    "    print(\"\\nüîç Generating Java Recommendations...\")\n",
    "    \n",
    "    schema = self.insights.get('schema_analysis', {})\n",
    "    quality = self.insights.get('data_quality', {})\n",
    "    content = self.insights.get('content_analysis', {})\n",
    "    \n",
    "    # Generate Java field recommendations\n",
    "    java_fields = {}\n",
    "    for col_name, col_info in schema.get('columns', {}).items():\n",
    "        field_name = self.to_camel_case(col_name)\n",
    "        \n",
    "        # Determine Java type and constraints\n",
    "        if 'string' in col_info['type'].lower() or 'object' in col_info['type'].lower():\n",
    "            # Get length recommendation from content analysis\n",
    "            max_length = 255  # default\n",
    "            if col_name in content.get('content_stats', {}):\n",
    "                max_length = max(500, content['content_stats'][col_name].get('percentile_95', 255))\n",
    "            \n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'String',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\", length = {max_length})',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "        \n",
    "        elif 'int' in col_info['type'].lower():\n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'Integer',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "        \n",
    "        elif 'bool' in col_info['type'].lower():\n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'Boolean',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "    \n",
    "    # Processing recommendations\n",
    "    processing_config = {\n",
    "        'recommended_batch_size': min(5000, max(1000, self.chunk_size // 10)),\n",
    "        'memory_per_batch_mb': round(self.get_memory_usage() / 10, 2),\n",
    "        'estimated_processing_time_hours': round((schema.get('total_rows', 0) / 10000) / 60, 2),\n",
    "        'high_priority_fields': quality.get('completeness_summary', {}).get('high_quality_fields', []),\n",
    "        'validation_required_fields': quality.get('completeness_summary', {}).get('low_quality_fields', [])\n",
    "    }\n",
    "    \n",
    "    self.insights['processing_recommendations'] = {\n",
    "        'java_fields': java_fields,\n",
    "        'processing_config': processing_config\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Java Recommendations Generated:\")\n",
    "    print(f\"   - Java Fields: {len(java_fields)}\")\n",
    "    print(f\"   - Recommended Batch Size: {processing_config['recommended_batch_size']}\")\n",
    "\n",
    "def to_camel_case(self, snake_str):\n",
    "    \"\"\"Convert snake_case to camelCase\"\"\"\n",
    "    components = snake_str.split('_')\n",
    "    return components[0] + ''.join(word.capitalize() for word in components[1:])\n",
    "\n",
    "def generate_database_schema(self):\n",
    "    \"\"\"Generate optimized database schema\"\"\"\n",
    "    print(\"\\nüîç Generating Database Schema...\")\n",
    "    \n",
    "    java_fields = self.insights.get('processing_recommendations', {}).get('java_fields', {})\n",
    "    \n",
    "    # Generate CREATE TABLE statement\n",
    "    create_table = \"CREATE TABLE profiles (\\n\"\n",
    "    create_table += \"    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\\n\"\n",
    "    \n",
    "    for field_name, field_info in list(java_fields.items())[:30]:  # Limit to first 30 fields for demo\n",
    "        col_name = field_info['original_column']\n",
    "        java_type = field_info['java_type']\n",
    "        null_pct = field_info['null_percentage']\n",
    "        \n",
    "        # Determine SQL type and constraints\n",
    "        if java_type == 'String':\n",
    "            length = field_info['jpa_annotation'].split('length = ')[1].split(')')[0] if 'length =' in field_info['jpa_annotation'] else '255'\n",
    "            sql_type = f\"VARCHAR({length})\"\n",
    "        elif java_type == 'Integer':\n",
    "            sql_type = \"INTEGER\"\n",
    "        elif java_type == 'Boolean':\n",
    "            sql_type = \"BOOLEAN\"\n",
    "        else:\n",
    "            sql_type = \"TEXT\"\n",
    "        \n",
    "        # Add NOT NULL for high-quality fields\n",
    "        nullable = \"\" if null_pct > 10 else \" NOT NULL\" if null_pct < 1 else \"\"\n",
    "        \n",
    "        create_table += f\"    {col_name} {sql_type}{nullable},\\n\"\n",
    "    \n",
    "    # Add vector embedding column\n",
    "    create_table += \"    embedding vector(1536),\\n\"\n",
    "    create_table += \"    created_at TIMESTAMP DEFAULT NOW(),\\n\"\n",
    "    create_table += \"    updated_at TIMESTAMP DEFAULT NOW()\\n\"\n",
    "    create_table += \");\"\n",
    "    \n",
    "    # Generate indexes\n",
    "    indexes = []\n",
    "    indexes.append(\"CREATE INDEX CONCURRENTLY profiles_embedding_hnsw_idx ON profiles USING hnsw (embedding vector_cosine_ops);\")\n",
    "    \n",
    "    # Add indexes for high-quality, commonly queried fields\n",
    "    high_quality_fields = self.insights.get('data_quality', {}).get('completeness_summary', {}).get('high_quality_fields', [])\n",
    "    for field in high_quality_fields[:5]:  # Top 5 fields\n",
    "        if field != 'id':\n",
    "            indexes.append(f\"CREATE INDEX CONCURRENTLY idx_profiles_{field} ON profiles({field});\")\n",
    "    \n",
    "    self.insights['database_schema'] = {\n",
    "        'create_table_sql': create_table,\n",
    "        'indexes_sql': indexes\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Database Schema Generated\")\n",
    "\n",
    "def save_insights(self):\n",
    "    \"\"\"Save all insights to files\"\"\"\n",
    "    print(\"\\nüíæ Saving Analysis Results...\")\n",
    "    \n",
    "    # Save JSON insights\n",
    "    insights_file = os.path.join(OUTPUT_DIR, 'linkedin_analysis_insights.json')\n",
    "    with open(insights_file, 'w') as f:\n",
    "        json.dump(self.insights, f, indent=2, default=str)\n",
    "    \n",
    "    # Save database schema\n",
    "    schema_file = os.path.join(OUTPUT_DIR, 'optimized_schema.sql')\n",
    "    with open(schema_file, 'w') as f:\n",
    "        f.write(self.insights.get('database_schema', {}).get('create_table_sql', ''))\n",
    "        f.write('\\n\\n-- Indexes\\n')\n",
    "        for index in self.insights.get('database_schema', {}).get('indexes_sql', []):\n",
    "            f.write(index + '\\n')\n",
    "    \n",
    "    print(f\"‚úÖ Results Saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"   - Insights: linkedin_analysis_insights.json\")\n",
    "    print(f\"   - Database Schema: optimized_schema.sql\")\n",
    "    \n",
    "    return insights_file, schema_file\n",
    "\n",
    "# Add methods to analyzer class\n",
    "LinkedInDataAnalyzer.generate_java_recommendations = generate_java_recommendations\n",
    "LinkedInDataAnalyzer.to_camel_case = to_camel_case\n",
    "LinkedInDataAnalyzer.generate_database_schema = generate_database_schema\n",
    "LinkedInDataAnalyzer.save_insights = save_insights\n",
    "\n",
    "# Now generate final insights and outputs\n",
    "analyzer.generate_java_recommendations()\n",
    "analyzer.generate_database_schema()\n",
    "insights_file, schema_file = analyzer.save_insights()\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ LINKEDIN DATASET ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "schema_info = analyzer.insights['schema_analysis']\n",
    "quality_info = analyzer.insights['data_quality']\n",
    "processing_info = analyzer.insights['processing_recommendations']['processing_config']\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   üìÅ File Size: {schema_info.get('file_size_gb', 0)} GB\")\n",
    "print(f\"   üìã Total Rows: {schema_info.get('total_rows', 0):,}\")\n",
    "print(f\"   üóÇÔ∏è  Total Columns: {schema_info.get('total_columns', 0)}\")\n",
    "print(f\"   üîç Rows Analyzed: {quality_info.get('total_rows_analyzed', 0):,}\")\n",
    "\n",
    "print(f\"\\nüéØ DATA QUALITY SUMMARY:\")\n",
    "completeness = quality_info.get('completeness_summary', {})\n",
    "print(f\"   üü¢ High Quality Fields: {len(completeness.get('high_quality_fields', []))}\")\n",
    "print(f\"   üü° Medium Quality Fields: {len(completeness.get('medium_quality_fields', []))}\")\n",
    "print(f\"   üî¥ Low Quality Fields: {len(completeness.get('low_quality_fields', []))}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è PROCESSING RECOMMENDATIONS:\")\n",
    "print(f\"   üì¶ Recommended Batch Size: {processing_info.get('recommended_batch_size', 0):,}\")\n",
    "print(f\"   üíæ Memory per Batch: {processing_info.get('memory_per_batch_mb', 0):.1f} MB\")\n",
    "print(f\"   ‚è±Ô∏è  Est. Processing Time: {processing_info.get('estimated_processing_time_hours', 0):.1f} hours\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   üìã Analysis Report: {insights_file}\")\n",
    "print(f\"   üóÉÔ∏è  Database Schema: {schema_file}\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   1. Review the generated analysis files\")\n",
    "print(\"   2. Use the database schema for your PostgreSQL setup\")\n",
    "print(\"   3. Apply the processing recommendations to your Java application\")\n",
    "print(\"   4. Use the identified high-quality fields for core functionality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "tnsu913qak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Methods added to LinkedInDataAnalyzer class\n"
     ]
    }
   ],
   "source": [
    "def generate_java_recommendations(self):\n",
    "    \"\"\"Generate Java model and configuration recommendations\"\"\"\n",
    "    print(\"\\nüîç Generating Java Recommendations...\")\n",
    "    \n",
    "    schema = self.insights.get('schema_analysis', {})\n",
    "    quality = self.insights.get('data_quality', {})\n",
    "    content = self.insights.get('content_analysis', {})\n",
    "    \n",
    "    # Generate Java field recommendations\n",
    "    java_fields = {}\n",
    "    for col_name, col_info in schema.get('columns', {}).items():\n",
    "        field_name = self.to_camel_case(col_name)\n",
    "        \n",
    "        # Determine Java type and constraints\n",
    "        if 'string' in col_info['type'].lower() or 'object' in col_info['type'].lower():\n",
    "            # Get length recommendation from content analysis\n",
    "            max_length = 255  # default\n",
    "            if col_name in content.get('content_stats', {}):\n",
    "                max_length = max(500, content['content_stats'][col_name].get('percentile_95', 255))\n",
    "            \n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'String',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\", length = {max_length})',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "        \n",
    "        elif 'int' in col_info['type'].lower():\n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'Integer',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "        \n",
    "        elif 'bool' in col_info['type'].lower():\n",
    "            java_fields[field_name] = {\n",
    "                'original_column': col_name,\n",
    "                'java_type': 'Boolean',\n",
    "                'jpa_annotation': f'@Column(name = \"{col_name}\")',\n",
    "                'nullable': col_info.get('nullable', True),\n",
    "                'null_percentage': quality.get('null_percentages', {}).get(col_name, 0)\n",
    "            }\n",
    "    \n",
    "    # Processing recommendations\n",
    "    processing_config = {\n",
    "        'recommended_batch_size': min(5000, max(1000, self.chunk_size // 10)),\n",
    "        'memory_per_batch_mb': round(self.get_memory_usage() / 10, 2),\n",
    "        'estimated_processing_time_hours': round((schema.get('total_rows', 0) / 10000) / 60, 2),\n",
    "        'high_priority_fields': quality.get('completeness_summary', {}).get('high_quality_fields', []),\n",
    "        'validation_required_fields': quality.get('completeness_summary', {}).get('low_quality_fields', [])\n",
    "    }\n",
    "    \n",
    "    self.insights['processing_recommendations'] = {\n",
    "        'java_fields': java_fields,\n",
    "        'processing_config': processing_config\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Java Recommendations Generated:\")\n",
    "    print(f\"   - Java Fields: {len(java_fields)}\")\n",
    "    print(f\"   - Recommended Batch Size: {processing_config['recommended_batch_size']}\")\n",
    "\n",
    "def to_camel_case(self, snake_str):\n",
    "    \"\"\"Convert snake_case to camelCase\"\"\"\n",
    "    components = snake_str.split('_')\n",
    "    return components[0] + ''.join(word.capitalize() for word in components[1:])\n",
    "\n",
    "def generate_database_schema(self):\n",
    "    \"\"\"Generate optimized database schema\"\"\"\n",
    "    print(\"\\nüîç Generating Database Schema...\")\n",
    "    \n",
    "    java_fields = self.insights.get('processing_recommendations', {}).get('java_fields', {})\n",
    "    \n",
    "    # Generate CREATE TABLE statement\n",
    "    create_table = \"CREATE TABLE profiles (\\n\"\n",
    "    create_table += \"    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\\n\"\n",
    "    \n",
    "    for field_name, field_info in list(java_fields.items())[:30]:  # Limit to first 30 fields for demo\n",
    "        col_name = field_info['original_column']\n",
    "        java_type = field_info['java_type']\n",
    "        null_pct = field_info['null_percentage']\n",
    "        \n",
    "        # Determine SQL type and constraints\n",
    "        if java_type == 'String':\n",
    "            length = field_info['jpa_annotation'].split('length = ')[1].split(')')[0] if 'length =' in field_info['jpa_annotation'] else '255'\n",
    "            sql_type = f\"VARCHAR({length})\"\n",
    "        elif java_type == 'Integer':\n",
    "            sql_type = \"INTEGER\"\n",
    "        elif java_type == 'Boolean':\n",
    "            sql_type = \"BOOLEAN\"\n",
    "        else:\n",
    "            sql_type = \"TEXT\"\n",
    "        \n",
    "        # Add NOT NULL for high-quality fields\n",
    "        nullable = \"\" if null_pct > 10 else \" NOT NULL\" if null_pct < 1 else \"\"\n",
    "        \n",
    "        create_table += f\"    {col_name} {sql_type}{nullable},\\n\"\n",
    "    \n",
    "    # Add vector embedding column\n",
    "    create_table += \"    embedding vector(1536),\\n\"\n",
    "    create_table += \"    created_at TIMESTAMP DEFAULT NOW(),\\n\"\n",
    "    create_table += \"    updated_at TIMESTAMP DEFAULT NOW()\\n\"\n",
    "    create_table += \");\"\n",
    "    \n",
    "    # Generate indexes\n",
    "    indexes = []\n",
    "    indexes.append(\"CREATE INDEX CONCURRENTLY profiles_embedding_hnsw_idx ON profiles USING hnsw (embedding vector_cosine_ops);\")\n",
    "    \n",
    "    # Add indexes for high-quality, commonly queried fields\n",
    "    high_quality_fields = self.insights.get('data_quality', {}).get('completeness_summary', {}).get('high_quality_fields', [])\n",
    "    for field in high_quality_fields[:5]:  # Top 5 fields\n",
    "        if field != 'id':\n",
    "            indexes.append(f\"CREATE INDEX CONCURRENTLY idx_profiles_{field} ON profiles({field});\")\n",
    "    \n",
    "    self.insights['database_schema'] = {\n",
    "        'create_table_sql': create_table,\n",
    "        'indexes_sql': indexes\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Database Schema Generated\")\n",
    "\n",
    "def save_insights(self):\n",
    "    \"\"\"Save all insights to files\"\"\"\n",
    "    print(\"\\nüíæ Saving Analysis Results...\")\n",
    "    \n",
    "    # Save JSON insights\n",
    "    insights_file = os.path.join(OUTPUT_DIR, 'linkedin_analysis_insights.json')\n",
    "    with open(insights_file, 'w') as f:\n",
    "        json.dump(self.insights, f, indent=2, default=str)\n",
    "    \n",
    "    # Save database schema\n",
    "    schema_file = os.path.join(OUTPUT_DIR, 'optimized_schema.sql')\n",
    "    with open(schema_file, 'w') as f:\n",
    "        f.write(self.insights.get('database_schema', {}).get('create_table_sql', ''))\n",
    "        f.write('\\n\\n-- Indexes\\n')\n",
    "        for index in self.insights.get('database_schema', {}).get('indexes_sql', []):\n",
    "            f.write(index + '\\n')\n",
    "    \n",
    "    print(f\"‚úÖ Results Saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"   - Insights: linkedin_analysis_insights.json\")\n",
    "    print(f\"   - Database Schema: optimized_schema.sql\")\n",
    "    \n",
    "    return insights_file, schema_file\n",
    "\n",
    "# Add methods to analyzer class\n",
    "LinkedInDataAnalyzer.generate_java_recommendations = generate_java_recommendations\n",
    "LinkedInDataAnalyzer.to_camel_case = to_camel_case\n",
    "LinkedInDataAnalyzer.generate_database_schema = generate_database_schema\n",
    "LinkedInDataAnalyzer.save_insights = save_insights\n",
    "\n",
    "print(\"‚úÖ Methods added to LinkedInDataAnalyzer class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hioyr52pq9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Generating Java Recommendations...\n",
      "‚úÖ Java Recommendations Generated:\n",
      "   - Java Fields: 62\n",
      "   - Recommended Batch Size: 5000\n",
      "\n",
      "üîç Generating Database Schema...\n",
      "‚úÖ Database Schema Generated\n",
      "\n",
      "üíæ Saving Analysis Results...\n",
      "‚úÖ Results Saved to: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n",
      "   - Insights: linkedin_analysis_insights.json\n",
      "   - Database Schema: optimized_schema.sql\n",
      "\n",
      "================================================================================\n",
      "üéâ LINKEDIN DATASET ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "   üìÅ File Size: 15.15 GB\n",
      "   üìã Total Rows: 51,352,619\n",
      "   üóÇÔ∏è  Total Columns: 62\n",
      "   üîç Rows Analyzed: 200,000\n",
      "\n",
      "üéØ DATA QUALITY SUMMARY:\n",
      "   üü¢ High Quality Fields: 12\n",
      "   üü° Medium Quality Fields: 7\n",
      "   üî¥ Low Quality Fields: 43\n",
      "\n",
      "‚öôÔ∏è PROCESSING RECOMMENDATIONS:\n",
      "   üì¶ Recommended Batch Size: 5,000\n",
      "   üíæ Memory per Batch: 67.3 MB\n",
      "   ‚è±Ô∏è  Est. Processing Time: 85.6 hours\n",
      "\n",
      "üìÅ OUTPUT FILES GENERATED:\n",
      "   üìã Analysis Report: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/linkedin_analysis_insights.json\n",
      "   üóÉÔ∏è  Database Schema: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/optimized_schema.sql\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1. Review the generated analysis files\n",
      "   2. Use the database schema for your PostgreSQL setup\n",
      "   3. Apply the processing recommendations to your Java application\n",
      "   4. Use the identified high-quality fields for core functionality\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Now run the complete analysis pipeline\n",
    "analyzer.generate_java_recommendations()\n",
    "analyzer.generate_database_schema()\n",
    "insights_file, schema_file = analyzer.save_insights()\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ LINKEDIN DATASET ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "schema_info = analyzer.insights['schema_analysis']\n",
    "quality_info = analyzer.insights['data_quality']\n",
    "processing_info = analyzer.insights['processing_recommendations']['processing_config']\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   üìÅ File Size: {schema_info.get('file_size_gb', 0)} GB\")\n",
    "print(f\"   üìã Total Rows: {schema_info.get('total_rows', 0):,}\")\n",
    "print(f\"   üóÇÔ∏è  Total Columns: {schema_info.get('total_columns', 0)}\")\n",
    "print(f\"   üîç Rows Analyzed: {quality_info.get('total_rows_analyzed', 0):,}\")\n",
    "\n",
    "print(f\"\\nüéØ DATA QUALITY SUMMARY:\")\n",
    "completeness = quality_info.get('completeness_summary', {})\n",
    "print(f\"   üü¢ High Quality Fields: {len(completeness.get('high_quality_fields', []))}\")\n",
    "print(f\"   üü° Medium Quality Fields: {len(completeness.get('medium_quality_fields', []))}\")\n",
    "print(f\"   üî¥ Low Quality Fields: {len(completeness.get('low_quality_fields', []))}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è PROCESSING RECOMMENDATIONS:\")\n",
    "print(f\"   üì¶ Recommended Batch Size: {processing_info.get('recommended_batch_size', 0):,}\")\n",
    "print(f\"   üíæ Memory per Batch: {processing_info.get('memory_per_batch_mb', 0):.1f} MB\")\n",
    "print(f\"   ‚è±Ô∏è  Est. Processing Time: {processing_info.get('estimated_processing_time_hours', 0):.1f} hours\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   üìã Analysis Report: {insights_file}\")\n",
    "print(f\"   üóÉÔ∏è  Database Schema: {schema_file}\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   1. Review the generated analysis files\")\n",
    "print(\"   2. Use the database schema for your PostgreSQL setup\")\n",
    "print(\"   3. Apply the processing recommendations to your Java application\")\n",
    "print(\"   4. Use the identified high-quality fields for core functionality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "r61h4uiow6n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STARTING ENHANCED ANALYSIS FOR SEMANTIC TALENT FINDER\n",
      "================================================================================\n",
      "\n",
      "üéØ Analyzing Skills Data for Semantic Search...\n",
      "   Found skills columns: ['Skills']\n",
      "‚úÖ Skills Analysis Complete:\n",
      "   - Skills Columns: 1\n",
      "   - Unique Skills Found: 1871\n",
      "   - Tech Skills: 18\n",
      "   - Soft Skills: 20\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED ANALYSIS FOR SEMANTIC TALENT FINDER PROJECT\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\nüöÄ STARTING ENHANCED ANALYSIS FOR SEMANTIC TALENT FINDER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_skills_data(self):\n",
    "    \"\"\"Deep analysis of skills data for semantic search optimization\"\"\"\n",
    "    print(\"\\nüéØ Analyzing Skills Data for Semantic Search...\")\n",
    "    \n",
    "    skills_insights = {\n",
    "        'skills_columns': [],\n",
    "        'skills_patterns': {},\n",
    "        'skills_standardization': {},\n",
    "        'top_skills': {},\n",
    "        'skills_combinations': {}\n",
    "    }\n",
    "    \n",
    "    chunk_count = 0\n",
    "    all_skills = []\n",
    "    skills_by_column = {}\n",
    "    \n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        # Identify skills-related columns\n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            \n",
    "            if chunk_count == 0:\n",
    "                # Find skills-related columns\n",
    "                skills_columns = [col for col in chunk_df.columns \n",
    "                                if any(keyword in col.lower() for keyword in \n",
    "                                      ['skill', 'competenc', 'expertise', 'technolog'])]\n",
    "                skills_insights['skills_columns'] = skills_columns\n",
    "                print(f\"   Found skills columns: {skills_columns}\")\n",
    "            \n",
    "            # Analyze skills content\n",
    "            for col in skills_insights['skills_columns'][:5]:  # Analyze top 5 skills columns\n",
    "                if col not in skills_by_column:\n",
    "                    skills_by_column[col] = []\n",
    "                \n",
    "                # Extract skills data\n",
    "                skills_data = chunk_df[col].dropna()\n",
    "                if len(skills_data) > 0:\n",
    "                    # Sample skills for analysis\n",
    "                    sample_skills = skills_data.iloc[:50].tolist()\n",
    "                    skills_by_column[col].extend(sample_skills)\n",
    "                    \n",
    "                    # Parse individual skills (assuming comma-separated or similar)\n",
    "                    for skill_entry in sample_skills:\n",
    "                        if isinstance(skill_entry, str):\n",
    "                            # Try different delimiters\n",
    "                            for delimiter in [',', ';', '|', '\\n']:\n",
    "                                if delimiter in skill_entry:\n",
    "                                    individual_skills = [s.strip() for s in skill_entry.split(delimiter)]\n",
    "                                    all_skills.extend(individual_skills)\n",
    "                                    break\n",
    "                            else:\n",
    "                                # No delimiter found, treat as single skill\n",
    "                                all_skills.append(skill_entry.strip())\n",
    "            \n",
    "            chunk_count += 1\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count >= 5:  # Limit for analysis\n",
    "                break\n",
    "        \n",
    "        # Analyze skills patterns\n",
    "        if all_skills:\n",
    "            from collections import Counter\n",
    "            skills_counter = Counter([skill.lower().strip() for skill in all_skills if skill.strip()])\n",
    "            \n",
    "            skills_insights['top_skills'] = dict(skills_counter.most_common(50))\n",
    "            skills_insights['total_unique_skills'] = len(skills_counter)\n",
    "            skills_insights['total_skill_mentions'] = sum(skills_counter.values())\n",
    "            \n",
    "            # Categorize skills\n",
    "            tech_keywords = ['python', 'java', 'javascript', 'react', 'node', 'sql', 'aws', 'docker']\n",
    "            soft_keywords = ['leadership', 'communication', 'management', 'teamwork', 'problem']\n",
    "            \n",
    "            tech_skills = [skill for skill in skills_counter.keys() \n",
    "                          if any(tech in skill for tech in tech_keywords)]\n",
    "            soft_skills = [skill for skill in skills_counter.keys() \n",
    "                          if any(soft in skill for soft in soft_keywords)]\n",
    "            \n",
    "            skills_insights['skills_categories'] = {\n",
    "                'technical_skills': tech_skills[:20],\n",
    "                'soft_skills': soft_skills[:20],\n",
    "                'tech_percentage': round(len(tech_skills) / len(skills_counter) * 100, 2),\n",
    "                'soft_percentage': round(len(soft_skills) / len(skills_counter) * 100, 2)\n",
    "            }\n",
    "        \n",
    "        self.insights['skills_analysis'] = skills_insights\n",
    "        \n",
    "        print(f\"‚úÖ Skills Analysis Complete:\")\n",
    "        print(f\"   - Skills Columns: {len(skills_insights['skills_columns'])}\")\n",
    "        print(f\"   - Unique Skills Found: {skills_insights.get('total_unique_skills', 0)}\")\n",
    "        print(f\"   - Tech Skills: {len(skills_insights.get('skills_categories', {}).get('technical_skills', []))}\")\n",
    "        print(f\"   - Soft Skills: {len(skills_insights.get('skills_categories', {}).get('soft_skills', []))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Skills analysis failed: {e}\")\n",
    "\n",
    "# Add method to analyzer class\n",
    "LinkedInDataAnalyzer.analyze_skills_data = analyze_skills_data\n",
    "\n",
    "# Run skills analysis\n",
    "analyzer.analyze_skills_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "jtgfvvds4o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Analyzing Text Content for Embedding Strategy...\n",
      "   Identified text fields for embeddings: ['Industry', 'Job title', 'Sub Role', 'Industry 2', 'Company Name', 'Company Industry', 'Company Website', 'Company Size', 'Company Founded', 'Company Linkedin Url']...\n",
      "‚úÖ Text Content Analysis Complete:\n",
      "   - Text Fields Identified: 25\n",
      "   - High-Value Fields: 0\n",
      "   - Recommended Primary Fields: []\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_content_for_embeddings(self):\n",
    "    \"\"\"Analyze text content to optimize embedding generation strategy\"\"\"\n",
    "    print(\"\\nüìù Analyzing Text Content for Embedding Strategy...\")\n",
    "    \n",
    "    text_content_insights = {\n",
    "        'text_fields': [],\n",
    "        'content_quality': {},\n",
    "        'embedding_strategy': {},\n",
    "        'text_combinations': {}\n",
    "    }\n",
    "    \n",
    "    chunk_count = 0\n",
    "    text_samples = {}\n",
    "    \n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            \n",
    "            if chunk_count == 0:\n",
    "                # Identify text fields suitable for embeddings\n",
    "                text_fields = []\n",
    "                for col in chunk_df.columns:\n",
    "                    col_lower = col.lower()\n",
    "                    # High-value text fields for semantic search\n",
    "                    if any(keyword in col_lower for keyword in \n",
    "                          ['summary', 'bio', 'description', 'about', 'headline', \n",
    "                           'experience', 'background', 'profile']):\n",
    "                        text_fields.append(col)\n",
    "                    # Job/role related fields\n",
    "                    elif any(keyword in col_lower for keyword in \n",
    "                            ['title', 'role', 'position', 'job']):\n",
    "                        text_fields.append(col)\n",
    "                    # Industry/company fields\n",
    "                    elif any(keyword in col_lower for keyword in \n",
    "                            ['industry', 'company', 'organization']):\n",
    "                        text_fields.append(col)\n",
    "                \n",
    "                text_content_insights['text_fields'] = text_fields\n",
    "                print(f\"   Identified text fields for embeddings: {text_fields[:10]}...\")\n",
    "            \n",
    "            # Sample text content\n",
    "            for field in text_content_insights['text_fields'][:15]:\n",
    "                if field not in text_samples:\n",
    "                    text_samples[field] = []\n",
    "                \n",
    "                field_data = chunk_df[field].dropna()\n",
    "                if len(field_data) > 0:\n",
    "                    samples = field_data.iloc[:20].tolist()\n",
    "                    text_samples[field].extend(samples)\n",
    "            \n",
    "            chunk_count += 1\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count >= 3:\n",
    "                break\n",
    "        \n",
    "        # Analyze text quality and patterns\n",
    "        for field, samples in text_samples.items():\n",
    "            if samples:\n",
    "                # Calculate text statistics\n",
    "                lengths = [len(str(sample)) for sample in samples if sample]\n",
    "                if lengths:\n",
    "                    text_content_insights['content_quality'][field] = {\n",
    "                        'avg_length': round(np.mean(lengths), 2),\n",
    "                        'max_length': max(lengths),\n",
    "                        'min_length': min(lengths),\n",
    "                        'sample_count': len(samples),\n",
    "                        'sample_text': samples[0][:200] if samples[0] else \"\",\n",
    "                        'information_density': round(np.mean(lengths) / 100, 2)  # chars per 100\n",
    "                    }\n",
    "        \n",
    "        # Generate embedding strategy recommendations\n",
    "        high_value_fields = []\n",
    "        medium_value_fields = []\n",
    "        \n",
    "        for field, quality in text_content_insights['content_quality'].items():\n",
    "            if quality['avg_length'] > 50 and quality['information_density'] > 0.5:\n",
    "                high_value_fields.append(field)\n",
    "            elif quality['avg_length'] > 20:\n",
    "                medium_value_fields.append(field)\n",
    "        \n",
    "        text_content_insights['embedding_strategy'] = {\n",
    "            'primary_text_fields': high_value_fields,\n",
    "            'secondary_text_fields': medium_value_fields,\n",
    "            'recommended_combination': ' | '.join(high_value_fields[:3]),\n",
    "            'embedding_length_estimate': sum([\n",
    "                text_content_insights['content_quality'].get(field, {}).get('avg_length', 0) \n",
    "                for field in high_value_fields[:3]\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        self.insights['text_content_analysis'] = text_content_insights\n",
    "        \n",
    "        print(f\"‚úÖ Text Content Analysis Complete:\")\n",
    "        print(f\"   - Text Fields Identified: {len(text_content_insights['text_fields'])}\")\n",
    "        print(f\"   - High-Value Fields: {len(high_value_fields)}\")\n",
    "        print(f\"   - Recommended Primary Fields: {high_value_fields[:3]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Text content analysis failed: {e}\")\n",
    "\n",
    "# Add method to analyzer\n",
    "LinkedInDataAnalyzer.analyze_text_content_for_embeddings = analyze_text_content_for_embeddings\n",
    "\n",
    "# Run text content analysis\n",
    "analyzer.analyze_text_content_for_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bx70jajsrnd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Analyzing Geographic Data Structure...\n",
      "   Found location fields: ['Location', 'Locality', 'Metro', 'Region', 'Company Location Name', 'Company Location Locality', 'Company Location Metro', 'Company Location Region', 'Company Location Geo', 'Company Location Street Address', 'Company Location Address Line 2', 'Company Location Postal Code', 'Company Location Country', 'Company Location Continent', 'Location Country', 'Location Continent', 'Location Geo']\n",
      "‚úÖ Geographic Analysis Complete:\n",
      "   - Location Fields: 17\n",
      "   - Hierarchy Levels: 6\n",
      "   - Primary Location: Location Geo\n",
      "\n",
      "üíº Analyzing Professional Experience Patterns...\n",
      "   Found experience fields: ['Job title', 'Sub Role', 'Birth Year', 'Years Experience']...\n",
      "‚úÖ Experience Analysis Complete:\n",
      "   - Experience Fields: 4\n",
      "   - Title Fields Analyzed: 1\n"
     ]
    }
   ],
   "source": [
    "def analyze_geographic_hierarchy(self):\n",
    "    \"\"\"Analyze geographic data structure and create location hierarchy\"\"\"\n",
    "    print(\"\\nüåç Analyzing Geographic Data Structure...\")\n",
    "    \n",
    "    geo_insights = {\n",
    "        'location_fields': [],\n",
    "        'location_hierarchy': {},\n",
    "        'location_patterns': {},\n",
    "        'geo_standardization': {}\n",
    "    }\n",
    "    \n",
    "    chunk_count = 0\n",
    "    location_data = {}\n",
    "    \n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            \n",
    "            if chunk_count == 0:\n",
    "                # Identify location fields\n",
    "                location_fields = [col for col in chunk_df.columns \n",
    "                                 if any(keyword in col.lower() for keyword in \n",
    "                                       ['location', 'city', 'state', 'country', 'region', \n",
    "                                        'metro', 'locality', 'continent'])]\n",
    "                geo_insights['location_fields'] = location_fields\n",
    "                print(f\"   Found location fields: {location_fields}\")\n",
    "            \n",
    "            # Sample location data\n",
    "            for field in geo_insights['location_fields']:\n",
    "                if field not in location_data:\n",
    "                    location_data[field] = []\n",
    "                \n",
    "                field_data = chunk_df[field].dropna()\n",
    "                if len(field_data) > 0:\n",
    "                    samples = field_data.iloc[:100].tolist()\n",
    "                    location_data[field].extend(samples)\n",
    "            \n",
    "            chunk_count += 1\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count >= 3:\n",
    "                break\n",
    "        \n",
    "        # Analyze location patterns\n",
    "        from collections import Counter\n",
    "        \n",
    "        for field, data in location_data.items():\n",
    "            if data:\n",
    "                # Count frequency of locations\n",
    "                location_counter = Counter(data)\n",
    "                geo_insights['location_patterns'][field] = {\n",
    "                    'top_locations': dict(location_counter.most_common(20)),\n",
    "                    'unique_count': len(location_counter),\n",
    "                    'total_entries': len(data)\n",
    "                }\n",
    "                \n",
    "                # Analyze location format patterns\n",
    "                sample_locations = [str(loc) for loc in data[:20]]\n",
    "                patterns = {\n",
    "                    'comma_separated': sum(1 for loc in sample_locations if ',' in loc),\n",
    "                    'has_state_codes': sum(1 for loc in sample_locations \n",
    "                                         if any(code in loc for code in [' CA', ' NY', ' TX', ' FL'])),\n",
    "                    'has_country': sum(1 for loc in sample_locations \n",
    "                                     if any(country in loc.upper() for country in ['USA', 'US', 'UNITED STATES'])),\n",
    "                    'avg_length': round(np.mean([len(loc) for loc in sample_locations]), 2)\n",
    "                }\n",
    "                geo_insights['location_patterns'][field]['format_patterns'] = patterns\n",
    "        \n",
    "        # Create location hierarchy mapping\n",
    "        hierarchy_mapping = {}\n",
    "        \n",
    "        # Try to map hierarchy relationships\n",
    "        for field in geo_insights['location_fields']:\n",
    "            field_lower = field.lower()\n",
    "            if 'continent' in field_lower:\n",
    "                hierarchy_mapping['continent'] = field\n",
    "            elif 'country' in field_lower:\n",
    "                hierarchy_mapping['country'] = field\n",
    "            elif 'region' in field_lower or 'state' in field_lower:\n",
    "                hierarchy_mapping['region'] = field\n",
    "            elif 'metro' in field_lower:\n",
    "                hierarchy_mapping['metro'] = field\n",
    "            elif 'locality' in field_lower or 'city' in field_lower:\n",
    "                hierarchy_mapping['locality'] = field\n",
    "            elif 'location' in field_lower and 'country' not in field_lower:\n",
    "                hierarchy_mapping['primary_location'] = field\n",
    "        \n",
    "        geo_insights['location_hierarchy'] = hierarchy_mapping\n",
    "        \n",
    "        # Generate standardization recommendations\n",
    "        geo_insights['geo_standardization'] = {\n",
    "            'primary_location_field': hierarchy_mapping.get('primary_location', 'Location'),\n",
    "            'hierarchy_fields': list(hierarchy_mapping.values()),\n",
    "            'needs_parsing': any('comma_separated' in geo_insights['location_patterns'].get(field, {}).get('format_patterns', {}) \n",
    "                               and geo_insights['location_patterns'][field]['format_patterns']['comma_separated'] > 10\n",
    "                               for field in geo_insights['location_fields']),\n",
    "            'standardization_priority': sorted(geo_insights['location_fields'], \n",
    "                                             key=lambda x: geo_insights['location_patterns'].get(x, {}).get('unique_count', 0), \n",
    "                                             reverse=True)[:5]\n",
    "        }\n",
    "        \n",
    "        self.insights['geographic_analysis'] = geo_insights\n",
    "        \n",
    "        print(f\"‚úÖ Geographic Analysis Complete:\")\n",
    "        print(f\"   - Location Fields: {len(geo_insights['location_fields'])}\")\n",
    "        print(f\"   - Hierarchy Levels: {len(hierarchy_mapping)}\")\n",
    "        print(f\"   - Primary Location: {geo_insights['geo_standardization']['primary_location_field']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Geographic analysis failed: {e}\")\n",
    "\n",
    "def analyze_professional_experience(self):\n",
    "    \"\"\"Analyze professional experience and career progression patterns\"\"\"\n",
    "    print(\"\\nüíº Analyzing Professional Experience Patterns...\")\n",
    "    \n",
    "    experience_insights = {\n",
    "        'experience_fields': [],\n",
    "        'experience_levels': {},\n",
    "        'career_patterns': {},\n",
    "        'seniority_indicators': {}\n",
    "    }\n",
    "    \n",
    "    chunk_count = 0\n",
    "    experience_data = {}\n",
    "    \n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            \n",
    "            if chunk_count == 0:\n",
    "                # Identify experience-related fields\n",
    "                experience_fields = []\n",
    "                for col in chunk_df.columns:\n",
    "                    col_lower = col.lower()\n",
    "                    if any(keyword in col_lower for keyword in \n",
    "                          ['experience', 'year', 'senior', 'junior', 'level', \n",
    "                           'title', 'position', 'role']):\n",
    "                        experience_fields.append(col)\n",
    "                \n",
    "                experience_insights['experience_fields'] = experience_fields\n",
    "                print(f\"   Found experience fields: {experience_fields[:10]}...\")\n",
    "            \n",
    "            # Sample experience data\n",
    "            for field in experience_insights['experience_fields'][:10]:\n",
    "                if field not in experience_data:\n",
    "                    experience_data[field] = []\n",
    "                \n",
    "                field_data = chunk_df[field].dropna()\n",
    "                if len(field_data) > 0:\n",
    "                    samples = field_data.iloc[:50].tolist()\n",
    "                    experience_data[field].extend(samples)\n",
    "            \n",
    "            chunk_count += 1\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count >= 3:\n",
    "                break\n",
    "        \n",
    "        # Analyze seniority patterns in job titles\n",
    "        title_fields = [field for field in experience_insights['experience_fields'] \n",
    "                       if 'title' in field.lower() or 'position' in field.lower()]\n",
    "        \n",
    "        seniority_keywords = {\n",
    "            'entry': ['intern', 'junior', 'entry', 'associate', 'trainee'],\n",
    "            'mid': ['mid', 'specialist', 'analyst', 'coordinator'],\n",
    "            'senior': ['senior', 'lead', 'principal', 'staff'],\n",
    "            'management': ['manager', 'director', 'head', 'chief', 'vp', 'vice president'],\n",
    "            'executive': ['ceo', 'cto', 'cfo', 'president', 'founder']\n",
    "        }\n",
    "        \n",
    "        seniority_analysis = {}\n",
    "        for field in title_fields[:3]:  # Analyze top 3 title fields\n",
    "            if field in experience_data:\n",
    "                titles = [str(title).lower() for title in experience_data[field]]\n",
    "                \n",
    "                level_counts = {}\n",
    "                for level, keywords in seniority_keywords.items():\n",
    "                    count = sum(1 for title in titles \n",
    "                              if any(keyword in title for keyword in keywords))\n",
    "                    level_counts[level] = count\n",
    "                \n",
    "                seniority_analysis[field] = level_counts\n",
    "        \n",
    "        experience_insights['seniority_indicators'] = seniority_analysis\n",
    "        \n",
    "        self.insights['experience_analysis'] = experience_insights\n",
    "        \n",
    "        print(f\"‚úÖ Experience Analysis Complete:\")\n",
    "        print(f\"   - Experience Fields: {len(experience_insights['experience_fields'])}\")\n",
    "        print(f\"   - Title Fields Analyzed: {len(title_fields)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Experience analysis failed: {e}\")\n",
    "\n",
    "# Add methods to analyzer\n",
    "LinkedInDataAnalyzer.analyze_geographic_hierarchy = analyze_geographic_hierarchy\n",
    "LinkedInDataAnalyzer.analyze_professional_experience = analyze_professional_experience\n",
    "\n",
    "# Run geographic and experience analysis\n",
    "analyzer.analyze_geographic_hierarchy()\n",
    "analyzer.analyze_professional_experience()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eegolradin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè¢ Analyzing Industry and Company Data...\n",
      "   Industry fields: ['Industry', 'Industry 2', 'Company Industry']\n",
      "   Company fields: ['Company Name', 'Company Industry', 'Company Website', 'Company Size', 'Company Founded', 'Company Linkedin Url', 'Company Facebook Url', 'Company Twitter Url', 'Company Location Name', 'Company Location Locality', 'Company Location Metro', 'Company Location Region', 'Company Location Geo', 'Company Location Street Address', 'Company Location Address Line 2', 'Company Location Postal Code', 'Company Location Country', 'Company Location Continent']\n",
      "‚úÖ Industry & Company Analysis Complete:\n",
      "   - Industry Fields: 3\n",
      "   - Company Fields: 18\n"
     ]
    }
   ],
   "source": [
    "def analyze_industry_company_data(self):\n",
    "    \"\"\"Deep dive into industry and company data standardization\"\"\"\n",
    "    print(\"\\nüè¢ Analyzing Industry and Company Data...\")\n",
    "    \n",
    "    industry_insights = {\n",
    "        'industry_fields': [],\n",
    "        'company_fields': [],\n",
    "        'industry_standardization': {},\n",
    "        'company_patterns': {},\n",
    "        'business_intelligence': {}\n",
    "    }\n",
    "    \n",
    "    chunk_count = 0\n",
    "    industry_data = {}\n",
    "    company_data = {}\n",
    "    \n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            \n",
    "            if chunk_count == 0:\n",
    "                # Identify industry and company fields\n",
    "                industry_fields = [col for col in chunk_df.columns \n",
    "                                 if 'industry' in col.lower()]\n",
    "                company_fields = [col for col in chunk_df.columns \n",
    "                                if 'company' in col.lower() or 'organization' in col.lower()]\n",
    "                \n",
    "                industry_insights['industry_fields'] = industry_fields\n",
    "                industry_insights['company_fields'] = company_fields\n",
    "                print(f\"   Industry fields: {industry_fields}\")\n",
    "                print(f\"   Company fields: {company_fields}\")\n",
    "            \n",
    "            # Sample industry data\n",
    "            for field in industry_insights['industry_fields']:\n",
    "                if field not in industry_data:\n",
    "                    industry_data[field] = []\n",
    "                \n",
    "                field_data = chunk_df[field].dropna()\n",
    "                if len(field_data) > 0:\n",
    "                    samples = field_data.iloc[:100].tolist()\n",
    "                    industry_data[field].extend(samples)\n",
    "            \n",
    "            # Sample company data\n",
    "            for field in industry_insights['company_fields']:\n",
    "                if field not in company_data:\n",
    "                    company_data[field] = []\n",
    "                \n",
    "                field_data = chunk_df[field].dropna()\n",
    "                if len(field_data) > 0:\n",
    "                    samples = field_data.iloc[:50].tolist()\n",
    "                    company_data[field].extend(samples)\n",
    "            \n",
    "            chunk_count += 1\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count >= 3:\n",
    "                break\n",
    "        \n",
    "        # Analyze industry standardization needs\n",
    "        from collections import Counter\n",
    "        \n",
    "        for field, data in industry_data.items():\n",
    "            if data:\n",
    "                industry_counter = Counter([str(ind).strip().title() for ind in data])\n",
    "                \n",
    "                # Group similar industries\n",
    "                tech_industries = [ind for ind in industry_counter.keys() \n",
    "                                 if any(keyword in ind.lower() for keyword in \n",
    "                                       ['technology', 'software', 'tech', 'it', 'computer'])]\n",
    "                finance_industries = [ind for ind in industry_counter.keys() \n",
    "                                    if any(keyword in ind.lower() for keyword in \n",
    "                                          ['finance', 'financial', 'bank', 'investment'])]\n",
    "                healthcare_industries = [ind for ind in industry_counter.keys() \n",
    "                                       if any(keyword in ind.lower() for keyword in \n",
    "                                             ['health', 'medical', 'pharma', 'hospital'])]\n",
    "                \n",
    "                industry_insights['industry_standardization'][field] = {\n",
    "                    'top_industries': dict(industry_counter.most_common(30)),\n",
    "                    'total_unique': len(industry_counter),\n",
    "                    'industry_clusters': {\n",
    "                        'technology': tech_industries[:10],\n",
    "                        'finance': finance_industries[:10],\n",
    "                        'healthcare': healthcare_industries[:10]\n",
    "                    },\n",
    "                    'standardization_needed': len(industry_counter) > 1000\n",
    "                }\n",
    "        \n",
    "        # Analyze company patterns\n",
    "        for field, data in company_data.items():\n",
    "            if data:\n",
    "                company_counter = Counter([str(comp).strip() for comp in data])\n",
    "                \n",
    "                # Identify Fortune 500 indicators (common large companies)\n",
    "                large_company_indicators = ['microsoft', 'google', 'amazon', 'apple', \n",
    "                                          'facebook', 'meta', 'netflix', 'tesla', 'ibm']\n",
    "                large_companies = [comp for comp in company_counter.keys() \n",
    "                                 if any(indicator in comp.lower() for indicator in large_company_indicators)]\n",
    "                \n",
    "                industry_insights['company_patterns'][field] = {\n",
    "                    'top_companies': dict(company_counter.most_common(20)),\n",
    "                    'total_unique': len(company_counter),\n",
    "                    'large_companies_found': large_companies[:10],\n",
    "                    'avg_company_name_length': round(np.mean([len(comp) for comp in company_counter.keys()]), 2)\n",
    "                }\n",
    "        \n",
    "        # Generate business intelligence insights\n",
    "        industry_insights['business_intelligence'] = {\n",
    "            'dominant_industries': [],\n",
    "            'company_size_indicators': [],\n",
    "            'market_insights': {}\n",
    "        }\n",
    "        \n",
    "        # Find dominant industries across all industry fields\n",
    "        all_industries = []\n",
    "        for field_data in industry_data.values():\n",
    "            all_industries.extend(field_data)\n",
    "        \n",
    "        if all_industries:\n",
    "            all_industry_counter = Counter([str(ind).strip().title() for ind in all_industries])\n",
    "            industry_insights['business_intelligence']['dominant_industries'] = dict(all_industry_counter.most_common(15))\n",
    "        \n",
    "        self.insights['industry_company_analysis'] = industry_insights\n",
    "        \n",
    "        print(f\"‚úÖ Industry & Company Analysis Complete:\")\n",
    "        print(f\"   - Industry Fields: {len(industry_insights['industry_fields'])}\")\n",
    "        print(f\"   - Company Fields: {len(industry_insights['company_fields'])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Industry/company analysis failed: {e}\")\n",
    "\n",
    "# Add method to analyzer\n",
    "LinkedInDataAnalyzer.analyze_industry_company_data = analyze_industry_company_data\n",
    "\n",
    "# Run industry and company analysis\n",
    "analyzer.analyze_industry_company_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6xtk29r9acg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóÑÔ∏è Generating Enhanced Database Schema for Semantic Talent Finder...\n",
      "‚úÖ Enhanced Database Schema Generated:\n",
      "   - Optimized for semantic search\n",
      "   - 18 specialized indexes\n",
      "   - Skills standardization support\n",
      "   - Geographic hierarchy optimization\n",
      "   - Full-text search integration\n",
      "\\nüíæ Saving Enhanced Analysis Results...\n",
      "‚úÖ Enhanced Results Saved:\n",
      "   - Complete Analysis: enhanced_linkedin_analysis.json\n",
      "   - Production Schema: semantic_talent_finder_schema.sql\n",
      "   - Processing Guide: data_processing_guide.json\n"
     ]
    }
   ],
   "source": [
    "def generate_enhanced_database_schema(self):\n",
    "    \"\"\"Generate production-ready database schema optimized for semantic search\"\"\"\n",
    "    print(\"\\nüóÑÔ∏è Generating Enhanced Database Schema for Semantic Talent Finder...\")\n",
    "    \n",
    "    # Collect all analysis insights\n",
    "    schema_analysis = self.insights.get('schema_analysis', {})\n",
    "    quality_analysis = self.insights.get('data_quality', {})\n",
    "    skills_analysis = self.insights.get('skills_analysis', {})\n",
    "    text_analysis = self.insights.get('text_content_analysis', {})\n",
    "    geo_analysis = self.insights.get('geographic_analysis', {})\n",
    "    experience_analysis = self.insights.get('experience_analysis', {})\n",
    "    industry_analysis = self.insights.get('industry_company_analysis', {})\n",
    "    \n",
    "    # Generate optimized CREATE TABLE statement\n",
    "    create_table = \"-- Semantic Talent Finder - Production Database Schema\\\\n\"\n",
    "    create_table += \"-- Generated from LinkedIn dataset analysis\\\\n\"\n",
    "    create_table += \"-- Optimized for vector similarity search and AI-powered matching\\\\n\\\\n\"\n",
    "    \n",
    "    # Enable required extensions\n",
    "    create_table += \"-- Enable required PostgreSQL extensions\\\\n\"\n",
    "    create_table += \"CREATE EXTENSION IF NOT EXISTS vector;\\\\n\"\n",
    "    create_table += \"CREATE EXTENSION IF NOT EXISTS pg_trgm;\\\\n\"\n",
    "    create_table += \"CREATE EXTENSION IF NOT EXISTS btree_gin;\\\\n\\\\n\"\n",
    "    \n",
    "    # Main profiles table\n",
    "    create_table += \"-- Main profiles table with semantic search optimization\\\\n\"\n",
    "    create_table += \"CREATE TABLE profiles (\\\\n\"\n",
    "    create_table += \"    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\\\\n\"\n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Core Identity Fields (High Quality - Low Null %)\\\\n\"\n",
    "    \n",
    "    # Add high-quality core fields\n",
    "    high_quality_fields = quality_analysis.get('completeness_summary', {}).get('high_quality_fields', [])\n",
    "    \n",
    "    core_identity_mapping = {\n",
    "        'Full name': ('full_name', 'VARCHAR(500)', 'NOT NULL'),\n",
    "        'First Name': ('first_name', 'VARCHAR(100)', 'NOT NULL'),\n",
    "        'Last Name': ('last_name', 'VARCHAR(100)', 'NOT NULL'),\n",
    "        'LinkedIn Url': ('linkedin_url', 'VARCHAR(500)', 'NOT NULL'),\n",
    "        'LinkedIn Username': ('linkedin_username', 'VARCHAR(100)', 'NOT NULL')\n",
    "    }\n",
    "    \n",
    "    for original_field, (db_field, data_type, constraint) in core_identity_mapping.items():\n",
    "        if original_field in high_quality_fields:\n",
    "            create_table += f\"    {db_field} {data_type} {constraint},\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Professional Information\\\\n\"\n",
    "    \n",
    "    # Professional fields based on text analysis\n",
    "    text_fields = text_analysis.get('text_fields', [])\n",
    "    professional_mapping = {\n",
    "        'Job title': ('job_title', 'VARCHAR(300)'),\n",
    "        'Industry': ('industry', 'VARCHAR(200)'),\n",
    "        'Company Name': ('company_name', 'VARCHAR(300)'),\n",
    "        'Company Industry': ('company_industry', 'VARCHAR(200)')\n",
    "    }\n",
    "    \n",
    "    for original_field, (db_field, data_type) in professional_mapping.items():\n",
    "        null_pct = quality_analysis.get('null_percentages', {}).get(original_field, 100)\n",
    "        constraint = ' NOT NULL' if null_pct < 1 else ''\n",
    "        create_table += f\"    {db_field} {data_type}{constraint},\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Geographic Information (Hierarchical)\\\\n\"\n",
    "    \n",
    "    # Geographic fields based on hierarchy analysis\n",
    "    geo_hierarchy = geo_analysis.get('location_hierarchy', {})\n",
    "    geographic_mapping = [\n",
    "        ('location', 'VARCHAR(500)', 'Primary location string'),\n",
    "        ('locality', 'VARCHAR(200)', 'City/locality'),\n",
    "        ('region', 'VARCHAR(200)', 'State/region'),  \n",
    "        ('country', 'VARCHAR(100)', 'Country'),\n",
    "        ('continent', 'VARCHAR(50)', 'Continent'),\n",
    "        ('metro_area', 'VARCHAR(200)', 'Metropolitan area')\n",
    "    ]\n",
    "    \n",
    "    for db_field, data_type, comment in geographic_mapping:\n",
    "        original_field = next((k for k, v in geo_hierarchy.items() if v.lower().replace(' ', '_') == db_field), None)\n",
    "        if original_field or db_field == 'location':\n",
    "            null_pct = quality_analysis.get('null_percentages', {}).get('Location', 100)\n",
    "            constraint = ' NOT NULL' if null_pct < 1 and db_field == 'location' else ''\n",
    "            create_table += f\"    {db_field} {data_type}{constraint}, -- {comment}\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Skills and Expertise (Optimized for Search)\\\\n\"\n",
    "    \n",
    "    # Skills based on skills analysis\n",
    "    skills_columns = skills_analysis.get('skills_columns', [])\n",
    "    if skills_columns:\n",
    "        create_table += \"    skills TEXT[], -- Array of skills for efficient querying\\\\n\"\n",
    "        create_table += \"    skills_text TEXT, -- Concatenated skills for full-text search\\\\n\"\n",
    "        create_table += \"    technical_skills TEXT[], -- Technical skills subset\\\\n\"\n",
    "        create_table += \"    soft_skills TEXT[], -- Soft skills subset\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Experience and Seniority\\\\n\"\n",
    "    create_table += \"    years_experience INTEGER, -- Calculated years of experience\\\\n\"\n",
    "    create_table += \"    experience_level VARCHAR(50), -- entry, mid, senior, management, executive\\\\n\"\n",
    "    create_table += \"    seniority_score INTEGER, -- Computed seniority score (0-100)\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Computed Text Fields for Semantic Search\\\\n\"\n",
    "    \n",
    "    # Text fields optimized for embeddings based on text analysis\n",
    "    primary_fields = text_analysis.get('embedding_strategy', {}).get('primary_text_fields', [])\n",
    "    create_table += \"    headline VARCHAR(500), -- Professional headline/summary\\\\n\"\n",
    "    create_table += \"    professional_summary TEXT, -- Combined professional description\\\\n\"\n",
    "    create_table += \"    searchable_content TEXT, -- Optimized content for embedding generation\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Contact Information (Optional)\\\\n\"\n",
    "    create_table += \"    email VARCHAR(320), -- RFC 5322 compliant length\\\\n\"\n",
    "    create_table += \"    phone VARCHAR(50),\\\\n\"\n",
    "    create_table += \"    mobile VARCHAR(50),\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Social Media\\\\n\"\n",
    "    create_table += \"    facebook_url VARCHAR(500),\\\\n\"\n",
    "    create_table += \"    twitter_url VARCHAR(500),\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Company Details\\\\n\"\n",
    "    create_table += \"    company_website VARCHAR(500),\\\\n\"\n",
    "    create_table += \"    company_size VARCHAR(100),\\\\n\"\n",
    "    create_table += \"    company_founded VARCHAR(20),\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- AI/ML Fields\\\\n\"\n",
    "    create_table += \"    embedding vector(1536) NOT NULL, -- OpenAI text-embedding-3-small\\\\n\"\n",
    "    create_table += \"    embedding_version VARCHAR(20) DEFAULT 'v1.0', -- Track embedding model version\\\\n\"\n",
    "    create_table += \"    content_hash VARCHAR(64), -- SHA-256 hash for detecting changes\\\\n\"\n",
    "    \n",
    "    create_table += \"    \\\\n\"\n",
    "    create_table += \"    -- Metadata\\\\n\"\n",
    "    create_table += \"    data_source VARCHAR(100) DEFAULT 'linkedin',\\\\n\"\n",
    "    create_table += \"    data_quality_score DECIMAL(3,2), -- 0.00 to 1.00\\\\n\"\n",
    "    create_table += \"    last_profile_update TIMESTAMP,\\\\n\"\n",
    "    create_table += \"    created_at TIMESTAMPTZ DEFAULT NOW(),\\\\n\"\n",
    "    create_table += \"    updated_at TIMESTAMPTZ DEFAULT NOW()\\\\n\"\n",
    "    create_table += \");\\\\n\\\\n\"\n",
    "    \n",
    "    # Generate optimized indexes\n",
    "    indexes = []\n",
    "    indexes.append(\"-- Vector Similarity Index (HNSW for fast approximate search)\")\n",
    "    indexes.append(\"CREATE INDEX profiles_embedding_hnsw_idx ON profiles USING hnsw (embedding vector_cosine_ops)\")\n",
    "    indexes.append(\"    WITH (m = 16, ef_construction = 64);\")\n",
    "    indexes.append(\"\")\n",
    "    \n",
    "    indexes.append(\"-- Core Identity Indexes\")\n",
    "    indexes.append(\"CREATE UNIQUE INDEX profiles_linkedin_username_idx ON profiles(linkedin_username);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_full_name_idx ON profiles(full_name);\") \n",
    "    indexes.append(\"CREATE INDEX profiles_name_trgm_idx ON profiles USING gin (full_name gin_trgm_ops);\")\n",
    "    indexes.append(\"\")\n",
    "    \n",
    "    indexes.append(\"-- Professional Search Indexes\")\n",
    "    indexes.append(\"CREATE INDEX profiles_job_title_idx ON profiles(job_title);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_industry_idx ON profiles(industry);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_company_idx ON profiles(company_name);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_experience_level_idx ON profiles(experience_level);\")\n",
    "    indexes.append(\"\")\n",
    "    \n",
    "    indexes.append(\"-- Geographic Search Indexes\")\n",
    "    indexes.append(\"CREATE INDEX profiles_location_idx ON profiles(location);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_region_idx ON profiles(region);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_country_idx ON profiles(country);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_geo_hierarchy_idx ON profiles(country, region, locality);\")\n",
    "    indexes.append(\"\")\n",
    "    \n",
    "    indexes.append(\"-- Skills Search Indexes\")\n",
    "    if skills_columns:\n",
    "        indexes.append(\"CREATE INDEX profiles_skills_gin_idx ON profiles USING gin (skills);\")\n",
    "        indexes.append(\"CREATE INDEX profiles_technical_skills_gin_idx ON profiles USING gin (technical_skills);\")\n",
    "        indexes.append(\"CREATE INDEX profiles_skills_text_trgm_idx ON profiles USING gin (skills_text gin_trgm_ops);\")\n",
    "        indexes.append(\"\")\n",
    "    \n",
    "    indexes.append(\"-- Full-Text Search Index\")\n",
    "    indexes.append(\"CREATE INDEX profiles_searchable_content_fts_idx ON profiles USING gin (to_tsvector('english', searchable_content));\")\n",
    "    indexes.append(\"\")\n",
    "    \n",
    "    indexes.append(\"-- Performance Indexes\")\n",
    "    indexes.append(\"CREATE INDEX profiles_quality_score_idx ON profiles(data_quality_score) WHERE data_quality_score >= 0.7;\")\n",
    "    indexes.append(\"CREATE INDEX profiles_updated_at_idx ON profiles(updated_at);\")\n",
    "    indexes.append(\"CREATE INDEX profiles_compound_search_idx ON profiles(industry, experience_level, country) WHERE data_quality_score >= 0.7;\")\n",
    "    \n",
    "    # Additional tables for semantic search optimization\n",
    "    additional_tables = []\n",
    "    additional_tables.append(\"\\\\n-- Skills lookup table for standardization\")\n",
    "    additional_tables.append(\"CREATE TABLE skills_dictionary (\")\n",
    "    additional_tables.append(\"    id SERIAL PRIMARY KEY,\")\n",
    "    additional_tables.append(\"    skill_name VARCHAR(200) NOT NULL UNIQUE,\")\n",
    "    additional_tables.append(\"    skill_category VARCHAR(100), -- technical, soft, industry\")\n",
    "    additional_tables.append(\"    skill_type VARCHAR(50), -- programming, framework, tool, etc.\")\n",
    "    additional_tables.append(\"    synonyms TEXT[], -- Alternative names\")\n",
    "    additional_tables.append(\"    popularity_score INTEGER DEFAULT 0,\")\n",
    "    additional_tables.append(\"    created_at TIMESTAMPTZ DEFAULT NOW()\")\n",
    "    additional_tables.append(\");\")\n",
    "    additional_tables.append(\"\")\n",
    "    additional_tables.append(\"CREATE INDEX skills_dictionary_name_idx ON skills_dictionary(skill_name);\")\n",
    "    additional_tables.append(\"CREATE INDEX skills_dictionary_category_idx ON skills_dictionary(skill_category);\")\n",
    "    additional_tables.append(\"\")\n",
    "    \n",
    "    additional_tables.append(\"-- Search analytics table\")\n",
    "    additional_tables.append(\"CREATE TABLE search_queries (\")\n",
    "    additional_tables.append(\"    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\")\n",
    "    additional_tables.append(\"    query_text TEXT NOT NULL,\")\n",
    "    additional_tables.append(\"    query_embedding vector(1536),\")\n",
    "    additional_tables.append(\"    user_session VARCHAR(100),\")\n",
    "    additional_tables.append(\"    results_count INTEGER,\")\n",
    "    additional_tables.append(\"    execution_time_ms INTEGER,\")\n",
    "    additional_tables.append(\"    filters_applied JSONB,\")\n",
    "    additional_tables.append(\"    created_at TIMESTAMPTZ DEFAULT NOW()\")\n",
    "    additional_tables.append(\");\")\n",
    "    additional_tables.append(\"\")\n",
    "    additional_tables.append(\"CREATE INDEX search_queries_created_at_idx ON search_queries(created_at);\")\n",
    "    additional_tables.append(\"CREATE INDEX search_queries_embedding_idx ON search_queries USING hnsw (query_embedding vector_cosine_ops);\")\n",
    "    \n",
    "    # Combine all schema elements\n",
    "    complete_schema = create_table + \"\\\\n\".join(indexes) + \"\\\\n\" + \"\\\\n\".join(additional_tables)\n",
    "    \n",
    "    # Generate data processing recommendations\n",
    "    processing_recommendations = {\n",
    "        'embedding_generation': {\n",
    "            'primary_fields': text_analysis.get('embedding_strategy', {}).get('primary_text_fields', []),\n",
    "            'content_template': 'Professional: {job_title} at {company_name} | Industry: {industry} | Location: {location} | Skills: {skills_text}',\n",
    "            'max_content_length': 8000,  # OpenAI limit consideration\n",
    "            'preprocessing_steps': [\n",
    "                'Remove HTML tags and special characters',\n",
    "                'Normalize whitespace',\n",
    "                'Handle null values with defaults',\n",
    "                'Truncate if exceeds max length'\n",
    "            ]\n",
    "        },\n",
    "        'data_quality_rules': {\n",
    "            'minimum_quality_score': 0.7,\n",
    "            'required_fields': ['full_name', 'linkedin_username'],\n",
    "            'embedding_generation_threshold': 0.5,\n",
    "            'content_validation': [\n",
    "                'full_name must not be empty',\n",
    "                'linkedin_username must be unique',\n",
    "                'at least one of job_title or industry must be present'\n",
    "            ]\n",
    "        },\n",
    "        'batch_processing': {\n",
    "            'recommended_batch_size': 5000,\n",
    "            'parallel_embedding_batch_size': 100,\n",
    "            'error_handling': 'skip_and_log',\n",
    "            'retry_logic': 'exponential_backoff'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    self.insights['enhanced_database_schema'] = {\n",
    "        'create_table_sql': complete_schema,\n",
    "        'processing_recommendations': processing_recommendations,\n",
    "        'schema_optimizations': {\n",
    "            'vector_index_type': 'HNSW',\n",
    "            'text_search_method': 'pg_trgm + GIN',\n",
    "            'skills_storage': 'TEXT[] arrays',\n",
    "            'geographic_hierarchy': 'normalized fields',\n",
    "            'full_text_search': 'tsvector with english config'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Enhanced Database Schema Generated:\")\n",
    "    print(f\"   - Optimized for semantic search\")\n",
    "    print(f\"   - {len([i for i in indexes if 'CREATE INDEX' in i])} specialized indexes\")\n",
    "    print(f\"   - Skills standardization support\")\n",
    "    print(f\"   - Geographic hierarchy optimization\")\n",
    "    print(f\"   - Full-text search integration\")\n",
    "\n",
    "def save_enhanced_insights(self):\n",
    "    \"\"\"Save enhanced analysis insights and schema\"\"\"\n",
    "    print(\"\\\\nüíæ Saving Enhanced Analysis Results...\")\n",
    "    \n",
    "    # Save comprehensive JSON insights\n",
    "    insights_file = os.path.join(OUTPUT_DIR, 'enhanced_linkedin_analysis.json')\n",
    "    with open(insights_file, 'w') as f:\n",
    "        json.dump(self.insights, f, indent=2, default=str)\n",
    "    \n",
    "    # Save production-ready schema\n",
    "    schema_file = os.path.join(OUTPUT_DIR, 'semantic_talent_finder_schema.sql')\n",
    "    with open(schema_file, 'w') as f:\n",
    "        schema_sql = self.insights.get('enhanced_database_schema', {}).get('create_table_sql', '')\n",
    "        f.write(schema_sql)\n",
    "    \n",
    "    # Save data processing guide\n",
    "    processing_file = os.path.join(OUTPUT_DIR, 'data_processing_guide.json')\n",
    "    processing_recommendations = self.insights.get('enhanced_database_schema', {}).get('processing_recommendations', {})\n",
    "    with open(processing_file, 'w') as f:\n",
    "        json.dump(processing_recommendations, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Enhanced Results Saved:\")\n",
    "    print(f\"   - Complete Analysis: enhanced_linkedin_analysis.json\")\n",
    "    print(f\"   - Production Schema: semantic_talent_finder_schema.sql\")\n",
    "    print(f\"   - Processing Guide: data_processing_guide.json\")\n",
    "    \n",
    "    return insights_file, schema_file, processing_file\n",
    "\n",
    "# Add methods to analyzer\n",
    "LinkedInDataAnalyzer.generate_enhanced_database_schema = generate_enhanced_database_schema\n",
    "LinkedInDataAnalyzer.save_enhanced_insights = save_enhanced_insights\n",
    "\n",
    "# Generate enhanced schema and save results\n",
    "analyzer.generate_enhanced_database_schema()\n",
    "insights_file, schema_file, processing_file = analyzer.save_enhanced_insights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rb6vz3a1tc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéâ ENHANCED LINKEDIN DATASET ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä COMPREHENSIVE DATASET ANALYSIS:\n",
      "   üìÅ File Size: 15.15 GB\n",
      "   üìã Total Rows: 51,352,619\n",
      "   üóÇÔ∏è  Total Columns: 62\n",
      "   üîç Sample Analyzed: 200,000\n",
      "\n",
      "üéØ DATA QUALITY INSIGHTS:\n",
      "   üü¢ High Quality Fields: 12\n",
      "   üü° Medium Quality Fields: 7\n",
      "   üî¥ Low Quality Fields: 43\n",
      "\n",
      "üéØ SKILLS ANALYSIS:\n",
      "   üìö Skills Columns Found: 1\n",
      "   üîß Unique Skills: 1,871\n",
      "   üíª Technical Skills: 18\n",
      "   ü§ù Soft Skills: 20\n",
      "\n",
      "üìù TEXT CONTENT ANALYSIS:\n",
      "   üìÑ Text Fields for Embeddings: 25\n",
      "   ‚≠ê High-Value Content Fields: 0\n",
      "   üìä Content Quality Fields: 15\n",
      "\n",
      "üåç GEOGRAPHIC ANALYSIS:\n",
      "   üó∫Ô∏è  Location Fields: 17\n",
      "   üèõÔ∏è  Hierarchy Levels: 6\n",
      "   üìç Primary Location Field: Location Geo\n",
      "\n",
      "üíº PROFESSIONAL EXPERIENCE:\n",
      "   üíª Experience Fields: 4\n",
      "   üè¢ Seniority Analysis: 1\n",
      "\n",
      "üè≠ INDUSTRY & COMPANY ANALYSIS:\n",
      "   üè¢ Industry Fields: 3\n",
      "   üèõÔ∏è  Company Fields: 18\n",
      "   üìä Top Industries: 15\n",
      "\n",
      "üóÑÔ∏è ENHANCED DATABASE SCHEMA:\n",
      "   üèóÔ∏è  Production-Ready Schema: ‚úÖ\n",
      "   üîç Vector Similarity Search: ‚úÖ\n",
      "   üìä Full-Text Search Integration: ‚úÖ\n",
      "   üéØ Skills Standardization: ‚úÖ\n",
      "   üåç Geographic Hierarchy: ‚úÖ\n",
      "\n",
      "üìÅ ENHANCED OUTPUT FILES:\n",
      "   üìã Complete Analysis: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/enhanced_linkedin_analysis.json\n",
      "   üóÉÔ∏è  Production Schema: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/semantic_talent_finder_schema.sql\n",
      "   üìñ Processing Guide: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output/data_processing_guide.json\n",
      "\n",
      "üöÄ SEMANTIC TALENT FINDER READINESS:\n",
      "   ‚úÖ Database schema optimized for 50M+ profiles\n",
      "   ‚úÖ Vector embeddings support with HNSW indexing\n",
      "   ‚úÖ Multi-dimensional search capabilities\n",
      "   ‚úÖ Skills-based semantic matching\n",
      "   ‚úÖ Geographic and experience filtering\n",
      "   ‚úÖ Full-text search integration\n",
      "   ‚úÖ Data quality scoring and validation\n",
      "\n",
      "üéØ RECOMMENDED NEXT STEPS:\n",
      "   1. Review enhanced_linkedin_analysis.json for detailed insights\n",
      "   2. Use semantic_talent_finder_schema.sql for database setup\n",
      "   3. Follow data_processing_guide.json for data ingestion\n",
      "   4. Implement embedding generation for searchable_content field\n",
      "   5. Set up vector similarity search endpoints\n",
      "   6. Configure skills standardization pipeline\n",
      "\n",
      "================================================================================\n",
      "üéâ ANALYSIS COMPLETE - READY FOR SEMANTIC TALENT FINDER IMPLEMENTATION!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL ENHANCED ANALYSIS SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ENHANCED LINKEDIN DATASET ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display comprehensive summary\n",
    "schema_info = analyzer.insights['schema_analysis']\n",
    "quality_info = analyzer.insights['data_quality']\n",
    "skills_info = analyzer.insights.get('skills_analysis', {})\n",
    "text_info = analyzer.insights.get('text_content_analysis', {})\n",
    "geo_info = analyzer.insights.get('geographic_analysis', {})\n",
    "experience_info = analyzer.insights.get('experience_analysis', {})\n",
    "industry_info = analyzer.insights.get('industry_company_analysis', {})\n",
    "\n",
    "print(f\"\\nüìä COMPREHENSIVE DATASET ANALYSIS:\")\n",
    "print(f\"   üìÅ File Size: {schema_info.get('file_size_gb', 0)} GB\")\n",
    "print(f\"   üìã Total Rows: {schema_info.get('total_rows', 0):,}\")\n",
    "print(f\"   üóÇÔ∏è  Total Columns: {schema_info.get('total_columns', 0)}\")\n",
    "print(f\"   üîç Sample Analyzed: {quality_info.get('total_rows_analyzed', 0):,}\")\n",
    "\n",
    "print(f\"\\nüéØ DATA QUALITY INSIGHTS:\")\n",
    "completeness = quality_info.get('completeness_summary', {})\n",
    "print(f\"   üü¢ High Quality Fields: {len(completeness.get('high_quality_fields', []))}\")\n",
    "print(f\"   üü° Medium Quality Fields: {len(completeness.get('medium_quality_fields', []))}\")\n",
    "print(f\"   üî¥ Low Quality Fields: {len(completeness.get('low_quality_fields', []))}\")\n",
    "\n",
    "print(f\"\\nüéØ SKILLS ANALYSIS:\")\n",
    "print(f\"   üìö Skills Columns Found: {len(skills_info.get('skills_columns', []))}\")\n",
    "print(f\"   üîß Unique Skills: {skills_info.get('total_unique_skills', 0):,}\")\n",
    "print(f\"   üíª Technical Skills: {len(skills_info.get('skills_categories', {}).get('technical_skills', []))}\")\n",
    "print(f\"   ü§ù Soft Skills: {len(skills_info.get('skills_categories', {}).get('soft_skills', []))}\")\n",
    "\n",
    "print(f\"\\nüìù TEXT CONTENT ANALYSIS:\")\n",
    "print(f\"   üìÑ Text Fields for Embeddings: {len(text_info.get('text_fields', []))}\")\n",
    "print(f\"   ‚≠ê High-Value Content Fields: {len(text_info.get('embedding_strategy', {}).get('primary_text_fields', []))}\")\n",
    "print(f\"   üìä Content Quality Fields: {len(text_info.get('content_quality', {}))}\")\n",
    "\n",
    "print(f\"\\nüåç GEOGRAPHIC ANALYSIS:\")\n",
    "print(f\"   üó∫Ô∏è  Location Fields: {len(geo_info.get('location_fields', []))}\")\n",
    "print(f\"   üèõÔ∏è  Hierarchy Levels: {len(geo_info.get('location_hierarchy', {}))}\")\n",
    "print(f\"   üìç Primary Location Field: {geo_info.get('geo_standardization', {}).get('primary_location_field', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nüíº PROFESSIONAL EXPERIENCE:\")\n",
    "print(f\"   üíª Experience Fields: {len(experience_info.get('experience_fields', []))}\")\n",
    "print(f\"   üè¢ Seniority Analysis: {len(experience_info.get('seniority_indicators', {}))}\")\n",
    "\n",
    "print(f\"\\nüè≠ INDUSTRY & COMPANY ANALYSIS:\")\n",
    "print(f\"   üè¢ Industry Fields: {len(industry_info.get('industry_fields', []))}\")\n",
    "print(f\"   üèõÔ∏è  Company Fields: {len(industry_info.get('company_fields', []))}\")\n",
    "print(f\"   üìä Top Industries: {len(industry_info.get('business_intelligence', {}).get('dominant_industries', {}))}\")\n",
    "\n",
    "print(f\"\\nüóÑÔ∏è ENHANCED DATABASE SCHEMA:\")\n",
    "enhanced_schema = analyzer.insights.get('enhanced_database_schema', {})\n",
    "print(f\"   üèóÔ∏è  Production-Ready Schema: ‚úÖ\")\n",
    "print(f\"   üîç Vector Similarity Search: ‚úÖ\")\n",
    "print(f\"   üìä Full-Text Search Integration: ‚úÖ\")\n",
    "print(f\"   üéØ Skills Standardization: ‚úÖ\")\n",
    "print(f\"   üåç Geographic Hierarchy: ‚úÖ\")\n",
    "\n",
    "print(f\"\\nüìÅ ENHANCED OUTPUT FILES:\")\n",
    "print(f\"   üìã Complete Analysis: {insights_file}\")\n",
    "print(f\"   üóÉÔ∏è  Production Schema: {schema_file}\")\n",
    "print(f\"   üìñ Processing Guide: {processing_file}\")\n",
    "\n",
    "print(f\"\\nüöÄ SEMANTIC TALENT FINDER READINESS:\")\n",
    "print(\"   ‚úÖ Database schema optimized for 50M+ profiles\")\n",
    "print(\"   ‚úÖ Vector embeddings support with HNSW indexing\")\n",
    "print(\"   ‚úÖ Multi-dimensional search capabilities\")\n",
    "print(\"   ‚úÖ Skills-based semantic matching\")\n",
    "print(\"   ‚úÖ Geographic and experience filtering\")\n",
    "print(\"   ‚úÖ Full-text search integration\")\n",
    "print(\"   ‚úÖ Data quality scoring and validation\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDED NEXT STEPS:\")\n",
    "print(\"   1. Review enhanced_linkedin_analysis.json for detailed insights\")\n",
    "print(\"   2. Use semantic_talent_finder_schema.sql for database setup\")\n",
    "print(\"   3. Follow data_processing_guide.json for data ingestion\")\n",
    "print(\"   4. Implement embedding generation for searchable_content field\")\n",
    "print(\"   5. Set up vector similarity search endpoints\")\n",
    "print(\"   6. Configure skills standardization pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ANALYSIS COMPLETE - READY FOR SEMANTIC TALENT FINDER IMPLEMENTATION!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
