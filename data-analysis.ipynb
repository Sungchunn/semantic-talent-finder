{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ade48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LinkedIn Parquet Dataset Analysis - Starting Setup...\n",
      "üìÅ Target File: /Users/chromatrical/CAREER/Local Linkedin DB/DataBase/USA_filtered.parquet\n",
      "‚≠ê Chunk Size: 50,000 rows\n",
      "üíæ Output Directory: /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n",
      "‚úÖ Setup Complete - Output directory created at /Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LinkedIn Parquet Dataset Analysis Script\n",
    "Analyzes 15.2GB parquet file with 20M rows for Semantic Talent Finder project\n",
    "\n",
    "This script processes large parquet files in chunks to:\n",
    "1. Extract schema and data type information\n",
    "2. Analyze data quality and completeness\n",
    "3. Generate insights for Java model optimization\n",
    "4. Provide database schema recommendations\n",
    "5. Configure processing pipeline parameters\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Configuration\n",
    "PARQUET_FILE = \"/Users/chromatrical/CAREER/Local Linkedin DB/DataBase/USA_filtered.parquet\"\n",
    "CHUNK_SIZE = 50000  # Process 50k rows at a time to manage memory\n",
    "OUTPUT_DIR = \"/Users/chromatrical/CAREER/Side Projects/semantic-talent-finder/data/analysis_output\"\n",
    "\n",
    "print(\"üöÄ LinkedIn Parquet Dataset Analysis - Starting Setup...\")\n",
    "print(f\"üìÅ Target File: {PARQUET_FILE}\")\n",
    "print(f\"‚≠ê Chunk Size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"üíæ Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Setup Complete - Output directory created at {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92kajbfvhzl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LinkedInDataAnalyzer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "class LinkedInDataAnalyzer:\n",
    "    def __init__(self, parquet_file_path, chunk_size=50000):\n",
    "        self.parquet_file = parquet_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.insights = {\n",
    "            'schema_analysis': {},\n",
    "            'data_quality': {},\n",
    "            'content_analysis': {},\n",
    "            'business_logic': {},\n",
    "            'processing_recommendations': {},\n",
    "            'database_schema': {}\n",
    "        }\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Monitor memory usage during processing\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    def analyze_parquet_schema(self):\n",
    "        \"\"\"Analyze parquet file schema and metadata\"\"\"\n",
    "        print(\"üîç Analyzing Parquet Schema...\")\n",
    "        \n",
    "        try:\n",
    "            # Read parquet metadata without loading data\n",
    "            parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "            schema = parquet_file.schema_arrow\n",
    "            metadata = parquet_file.metadata\n",
    "            \n",
    "            # Extract schema information\n",
    "            schema_info = {}\n",
    "            for i, field in enumerate(schema):\n",
    "                schema_info[field.name] = {\n",
    "                    'type': str(field.type),\n",
    "                    'nullable': field.nullable,\n",
    "                    'index': i\n",
    "                }\n",
    "            \n",
    "            self.insights['schema_analysis'] = {\n",
    "                'total_columns': len(schema),\n",
    "                'total_rows': metadata.num_rows,\n",
    "                'file_size_gb': round(os.path.getsize(self.parquet_file) / (1024**3), 2),\n",
    "                'columns': schema_info,\n",
    "                'column_names': [field.name for field in schema]\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Schema Analysis Complete:\")\n",
    "            print(f\"   - Total Rows: {metadata.num_rows:,}\")\n",
    "            print(f\"   - Total Columns: {len(schema)}\")\n",
    "            print(f\"   - File Size: {self.insights['schema_analysis']['file_size_gb']} GB\")\n",
    "            print(f\"   - Columns: {', '.join(list(schema_info.keys())[:10])}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Schema analysis failed: {e}\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = LinkedInDataAnalyzer(PARQUET_FILE, CHUNK_SIZE)\n",
    "print(\"üìä LinkedInDataAnalyzer initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "y988owpel2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing Parquet Schema...\n",
      "‚úÖ Schema Analysis Complete:\n",
      "   - Total Rows: 51,352,619\n",
      "   - Total Columns: 62\n",
      "   - File Size: 15.15 GB\n",
      "   - Columns: Full name, Industry, Job title, Sub Role, Industry 2, Emails, Mobile, Phone numbers, Company Name, Company Industry...\n",
      "\n",
      "üìã Schema Summary:\n",
      "Total Rows: 51,352,619\n",
      "Total Columns: 62\n",
      "File Size: 15.15 GB\n",
      "\n",
      "üîç Column Overview:\n",
      "   1. Full name                      | string          | nullable\n",
      "   2. Industry                       | string          | nullable\n",
      "   3. Job title                      | string          | nullable\n",
      "   4. Sub Role                       | string          | nullable\n",
      "   5. Industry 2                     | string          | nullable\n",
      "   6. Emails                         | string          | nullable\n",
      "   7. Mobile                         | string          | nullable\n",
      "   8. Phone numbers                  | string          | nullable\n",
      "   9. Company Name                   | string          | nullable\n",
      "  10. Company Industry               | string          | nullable\n",
      "  11. Company Website                | string          | nullable\n",
      "  12. Company Size                   | string          | nullable\n",
      "  13. Company Founded                | string          | nullable\n",
      "  14. Location                       | string          | nullable\n",
      "  15. Locality                       | string          | nullable\n",
      "  ... and 47 more columns\n"
     ]
    }
   ],
   "source": [
    "# Run schema analysis\n",
    "analyzer.analyze_parquet_schema()\n",
    "\n",
    "# Display schema results\n",
    "print(\"\\nüìã Schema Summary:\")\n",
    "schema_info = analyzer.insights['schema_analysis']\n",
    "print(f\"Total Rows: {schema_info.get('total_rows', 0):,}\")\n",
    "print(f\"Total Columns: {schema_info.get('total_columns', 0)}\")\n",
    "print(f\"File Size: {schema_info.get('file_size_gb', 0)} GB\")\n",
    "\n",
    "print(\"\\nüîç Column Overview:\")\n",
    "columns = schema_info.get('columns', {})\n",
    "for i, (col_name, col_info) in enumerate(list(columns.items())[:15]):  # Show first 15 columns\n",
    "    nullable = \"nullable\" if col_info.get('nullable', True) else \"not null\"\n",
    "    print(f\"  {i+1:2d}. {col_name:<30} | {col_info.get('type', 'unknown'):<15} | {nullable}\")\n",
    "\n",
    "if len(columns) > 15:\n",
    "    print(f\"  ... and {len(columns) - 15} more columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2vywwbzjadq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing Data Quality in Chunks...\n",
      "‚úÖ Data Quality Analysis Complete:\n",
      "   - Rows Analyzed: 200,000\n",
      "   - High Quality Fields: 12\n",
      "   - Low Quality Fields: 43\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_quality_chunked(self):\n",
    "    \"\"\"Analyze data quality in chunks to handle large file\"\"\"\n",
    "    print(\"\\nüîç Analyzing Data Quality in Chunks...\")\n",
    "    \n",
    "    # Initialize aggregators\n",
    "    null_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "    data_types = {}\n",
    "    \n",
    "    chunk_count = 0\n",
    "    total_rows_processed = 0\n",
    "    \n",
    "    try:\n",
    "        # Process file in chunks\n",
    "        parquet_file = pq.ParquetFile(self.parquet_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            chunk_count += 1\n",
    "            total_rows_processed += len(chunk_df)\n",
    "            \n",
    "            # Analyze each column\n",
    "            for column in chunk_df.columns:\n",
    "                # Count nulls\n",
    "                null_counts[column] += chunk_df[column].isnull().sum()\n",
    "                total_counts[column] += len(chunk_df)\n",
    "                \n",
    "                # Store data type\n",
    "                if column not in data_types:\n",
    "                    data_types[column] = str(chunk_df[column].dtype)\n",
    "            \n",
    "            # Memory management\n",
    "            del chunk_df\n",
    "            gc.collect()\n",
    "            \n",
    "            if chunk_count % 50 == 0:\n",
    "                print(f\"   Processed {chunk_count} chunks ({total_rows_processed:,} rows)\")\n",
    "                print(f\"   Memory usage: {self.get_memory_usage():.2f} MB\")\n",
    "            \n",
    "            # Limit analysis for demo - analyze first 200k rows\n",
    "            if chunk_count >= 4:\n",
    "                break\n",
    "        \n",
    "        # Calculate null percentages\n",
    "        null_percentages = {}\n",
    "        for column in null_counts:\n",
    "            null_percentages[column] = round((null_counts[column] / total_counts[column]) * 100, 2)\n",
    "        \n",
    "        self.insights['data_quality'] = {\n",
    "            'total_rows_analyzed': total_rows_processed,\n",
    "            'chunks_processed': chunk_count,\n",
    "            'null_counts': dict(null_counts),\n",
    "            'null_percentages': null_percentages,\n",
    "            'data_types': data_types,\n",
    "            'completeness_summary': {\n",
    "                'high_quality_fields': [col for col, pct in null_percentages.items() if pct < 5],\n",
    "                'medium_quality_fields': [col for col, pct in null_percentages.items() if 5 <= pct < 25],\n",
    "                'low_quality_fields': [col for col, pct in null_percentages.items() if pct >= 25]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Data Quality Analysis Complete:\")\n",
    "        print(f\"   - Rows Analyzed: {total_rows_processed:,}\")\n",
    "        print(f\"   - High Quality Fields: {len(self.insights['data_quality']['completeness_summary']['high_quality_fields'])}\")\n",
    "        print(f\"   - Low Quality Fields: {len(self.insights['data_quality']['completeness_summary']['low_quality_fields'])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data quality analysis failed: {e}\")\n",
    "\n",
    "# Add method to analyzer class\n",
    "LinkedInDataAnalyzer.analyze_data_quality_chunked = analyze_data_quality_chunked\n",
    "\n",
    "# Run data quality analysis\n",
    "analyzer.analyze_data_quality_chunked()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
